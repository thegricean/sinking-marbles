# TODO FOR WONKY MARBLES (manuscript)


- p. 10 rerun and rereport all analyses with four-step numbers

- p. 11 footnote describing other possible priors

- p. 12 we need to explain where oddness comes from – maybe there’s an easy graphical way of showing how for items with different priors, a ‘some’ utterance is more or less wonky, by showing the marginal probabilities of observing each utterance for these different items?

- p. 12 rerun and rereport all analyses with four-step numbers

- p. 14 put in here somewhere the relation between wonkiness and the comprehension data: for all and none, while there are huge changes in wonkiness by prior, we don’t expect this to show up in the comprehension data because the semantics of the utterances restricts the interpretation to just one state, regardless of the prior. but for ”some”, which has a weak semantics, wonkiness shifts the overall interpretation in a way that compresses the effect of the prior

- p. 19 rerun and rereport all analyses with four-step numbers

- p. 20 GD overhaul -- make sure you go into presupposition accommodation, judith t. also thinks this would be a good way to make this relevant for linguists

# QUESTIONS

- instead of presenting MSE, present some measure you get out of the bayesian model comparison between rrsa and wrsa. TODO: mh does bda.

- should we try to get quantitative fit for wonkiness? yes, by assuming softmax linking function  between model predictions and data. TODO: mh implements this as part of regular wonkiness model

- include a bit on why alternatives like null utterance model doesn't work (maybe build into a section that's combined with speaker wonkiness -- basically, alternatives to world wonkiness) 