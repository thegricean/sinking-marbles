// time webppl wonky-tfbt.wppl --require wonkyutils


// helper functions

var foreach = function(lst, fn) {
    var foreach_ = function(i) {
        if (i < lst.length) {
            fn(lst[i]);
            foreach_(i + 1);
        }
    };
    foreach_(0);
};


// use first row as (header) variable names
var dataFrame = function(rawCSV){
    return map(function(row){
        return _.object(_.zip(rawCSV[0],row))
    }, rawCSV.slice(1))
}

var marginalize = function(myERP, index){
  Enumerate(function(){
    var x = sample(myERP)
    return x[index]
  })
}


var modelScore = function(modelERP, data){
    return reduce(function(dataPoint, memo) {
                return memo + modelERP.score([], dataPoint);
            }, 0, data)
  }


// data

// var linkingFunction = function(marginalERP, measure, linkingBetaConcentration, wonkySoftmax){
// // var linkingFunction = cache(function(marginalERP, measure, sigma, wonkySoftmax){
//   Enumerate(function(){

//     // var queryStatement = sample(betaERP, [shape_alpha(posteriorProb, delta),
//     //                                        shape_beta(posteriorProb, delta)])

//     var discreteModelPredSliderVal = function(gamma, delta) {
//       var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
//       var discretizeBeta = function(gamma, delta){
//         var shape_alpha = gamma * delta
//         var shape_beta = (1-gamma) * delta
//         var betaPDF = function(x){
//           return Math.pow(x,shape_alpha-1)*
//               Math.pow((1-x),shape_beta-1)
//         }
//         return map(betaPDF, bins)
//       }
//       var quintileProbs = discretizeBeta(gamma,delta)
//       //return quintileProbs
//        var sliderVal = bins[discrete(quintileProbs)]
//        return sliderVal
//     }

var logitNoiseLogistic = function(prob, sigma, offset, scale, bins){
  var discreteGaussian=  map(function(d){
    return Math.exp(gaussianERP.score([wonkyutils.logit(prob), sigma], 
                                       wonkyutils.logit(d)))
  }, bins)
  var noisyLogit_p =  wonkyutils.logit(bins[discrete(discreteGaussian)])
  var logisitic_noisy_p = wonkyutils.logistic(noisyLogit_p, offset, scale)

 return wonkyutils.roundToBin(logisitic_noisy_p, bins)
}



var queryDictionary = {'comp_state':"world",
                        'comp_allprob':"world",
                        'wonkiness':"wonky"}



var discreteGaussian = function(mu, sigma, bins){
  return map(function(d){
      return Math.exp(gaussianERP.score([wonkyutils.logit(mu), sigma], 
                                       wonkyutils.logit(d)))
  }, bins)
}

var truncatedDiscreteGaussian = function(mu, sigma, bins){
  return map(function(d){
      return Math.exp(gaussianERP.score([mu, sigma], 
                                       d))
  }, bins)
}


var expectationPlusNoiseLink = function(marginalERP, sigma){
  var states = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
  var bins = [ 0.01,
  0.066,
  0.133,
  0.2,
  0.266,
  0.333,
  0.4,
  0.466,
  0.533,
  0.6,
  0.666,
  0.733,
  0.8,
  0.866,
  0.933,
  0.99 ]
  var expvalNormalized = expectation(marginalERP) / states[states.length-1]

  var expvalAvoidEnds = (expvalNormalized>0.99) ? 0.99 :
                                        (expvalNormalized < 0.01) ? 0.01 :
                                              expvalNormalized
  var noisyExpectationProbs = discreteGaussian(expvalAvoidEnds, sigma, bins)
  return states[discrete(noisyExpectationProbs)]
  // return noisyExpectationProbs
}

var expectationPlusNoiseLink2 = function(marginalERP, sigma){
    var states = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
    var expval = expectation(marginalERP)
    var noisyExpectationProbs = truncatedDiscreteGaussian(expval, sigma, states)
    return Enumerate(function(){

      return states[discrete(noisyExpectationProbs)]
    })
}



var guessingBehavior = function(measure, bins, states){
  return  (measure == "comp_allprob") ?  uniformDraw(bins) : 
          (measure == "comp_state") ?  uniformDraw(states) :
                                      flip(0.5) ? 1: 0 
}



var linkingFunction2 =  function(marginalERP, options){

  Enumerate(function(){

    var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
    var states = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
    var posteriorProb = options.measure == "comp_allprob" ? 
                                Math.exp(marginalERP.score([], 15)):
                               wonkyutils.softmax(Math.exp(marginalERP.score([], true)),options.softmax)
                               // Math.exp(marginalERP.score([], true))

    var posteriorProbAvoidEnds = (posteriorProb>0.99) ? 0.99 :
                                        (posteriorProb < 0.01) ? 0.01 :
                                              posteriorProb

    var queryStatement =  flip(options.phi) ?  guessingBehavior(options.measure, bins, states) :
                                    (options.measure == "comp_allprob") ? 
                                                    // wonkyutils.roundToBin(posteriorProbAvoidEnds, bins) :
                                                  // wonkyutils.roundToBin(wonkyutils.logitLogistic(posteriorProbAvoidEnds, offset, scale), bins) :
                                                   logitNoiseLogistic(posteriorProbAvoidEnds, options.sigma, options.offset, options.scale, bins) :
                                                   // posteriorProbAvoidEnds : 
                                     (options.measure == "comp_state") ? 
                                                    expectationPlusNoiseLink2(marginalERP, options.sigma) : 
                                                    // logitNoiseLogistic(posteriorProbAvoidEnds,wonkySigma, wonkyOffset, wonkyScale)
                                                    flip(posteriorProb) ? 1 : 0
                                                    // flip(wonkyutils.logitLogistic(posteriorProb, options.offset, options.scale))?1:0


    return queryStatement
})
}


var dfilepath = 'data/';
// var datafile = wonkyutils.readCSV(dfilepath+ 'empirical_binarywonky_allQ.csv').data
// var df_language = dataFrame(datafile.slice(0, datafile.length-1))
var languageData = wonkyutils.readData()

// var priorfile = wonkyutils.readCSV(dfilepath+ 'priors.csv').data
// // var priorfile = wonkyutils.readCSV(dfilepath+ 'priors_original.csv').data
// var dPriors = priorfile.slice(0,priorfile.length)
// var df_prior = _.object(map(function(lst){return [lst[0], map(function(x){return wonkyutils.wpParseFloat(x)}, lst.slice(1))]}, dPriors))

// var measures = _.uniq(_.pluck(df_language, "measure"))
// var items = _.uniq(_.pluck(df_language, "item"))
// var utterances = _.uniq(_.pluck(df_language, "utterance"))

var items = _.keys(languageData)

// var measures = _.uniq(_.pluck(df_language, "measure"))
var utterances = _.keys(languageData[items[0]])

var measures = _.keys(languageData[items[0]][utterances[0]])
// var measures = ["comp_state", "comp_allprob"]

// var measures = ["comp_state"]
// var utterances = ["Some", "short_filler", "long_filler"]
// var utterances = ["Some"]
// var utterances = ["short_filler", "long_filler"]

// var items = ['fell down card towers']
// var guessingLink =  function(marginalERP, phi){
//   Enumerate(function(){
//     var states = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
//     var queryStatement = flip(phi) ? uniformDraw(states) : sample(marginalERP)
//     return queryStatement
// })
// }

var prioriter = 50000
var priorburn = prioriter / 2

var priorERP = IncrementalMH(priorModel, prioriter, {verbose:true, burnin: priorburn})

var priorERPobject = _.object(map(function(i){
  return [i, marginalizeERP(priorERP, i)]
}, items))

// var priorERPobject = _.object(map(function(i){
//   return [i, marginalizeERP(priorERP, i).MAP()]
// }, items))

// // var outpriorfile = "results/priorBDA-binomials-phi_incrMH" + prioriter + "burn" + priorburn + ".csv"
// // wonkyutils.priorERPwriter(priorERP, outpriorfile)
// // console.log('wrote prior to file ... ' + outpriorfile)

var addGuessingNoise = function(predictionERP, phi){
  Enumerate(function(){
    var states = _.range(0,16)
    var guessing = flip(phi)
    var marbles =  guessing ? uniformDraw(states) : sample(predictionERP)
    return marbles
  })
}

var cooperateAndNot = function(predictionERP, uncooperativeERP, speakerCooperativity){
  Enumerate(function(){
    var cooperate = flip(speakerCooperativity)
    var marbles =  cooperate ? sample(predictionERP) : sample(uncooperativeERP)
    return marbles
  })
}



var dataAnalysis = function(){



      // Priors (continuous)
      var speakerOptimality = uniform(0,10)
      // var speakerOptimality = {
      //   "world":uniform(0,10),
      //   "wonky":uniform(0,10)
      // }
      // var speakerCooperativity = uniform(0,1)
      var speakerCooperativity = 1

      var wonkinessPrior = uniform(0,1)
      // var wonkinessPrior = 0
      // var wonkinessPrior = {
      //   "world": uniform(0,1),
      //   "wonky": uniform(0,1)
      // }
      // var phi = 0
      var phi = uniform(0,1)
      // var phi = 0.0001
      // var cooperativeMix = uniform(0,1)


      // var phi = {
      //   "world":uniform(0,1),
      //   "wonky":uniform(0,1)
      // }

      // var sigma = uniform(0.5,10)

      // var sigma = 0.3
      // var sigma_some = uniform(0.1,2)
      // var sigma_allnone = uniform(0.1,2)

      // console.log(measure)
      // var speakerOptimality = 

     // var sigma = measure=='comp_allprob' ? uniform(0.1,5) : 
     //             measure=='comp_state' ? uniform(0.1,2) :
     //                                     0
     // var sigma = 0
     // var softmax = measure=="wonkiness" ? uniform(0,10) : 1
     // var softmax = 1;
      // var linkingSigma = 0
      // var scale = measure=='comp_allprob' ? uniform(0,4) : 1
      // var scale = 1
      // var offset = measure=='comp_allprob' ? sample(gaussianDriftERP,[0,10]) : 0
      // var offset = 0
      // var scale = 1
      // var offset = 0




  foreach(items,
    function(item){

      // var phi = uniform(0,1)
      // var prior = df_prior[item]


      // var theta = sample(priorERPobject[item])
      // // for MAP estimate
      // var theta = priorERPobject[item]["val"]
      // var prior = priorERPobject[item]["val"]

      // var prior = map(function(x){
      //   return Math.exp(binomialERP.score([theta,15], x))
      // }, _.range(0,16))
      // var prior = sample(priorERPobject[item])
      var priorParameters = sample(priorERPobject[item])


      // console.log(item)
    foreach(utterances,
      function(utterance){

          // var sigma = utterance=="Some" ? sigma_some : sigma_allnone
          // console.log('pre predictionERP')

          var utt = ((utterance == "short_filler") | (utterance == "long_filler")) ? "mu" : utterance
          // var predictionERP = listener(utt, 
          //                               wonkinessPrior,
          //                               // wonkinessPrior[queryDictionary[measure]], 
          //                               // speakerOptimality[queryDictionary[measure]], 
          //                               speakerOptimality, 
          //                               speakerCooperativity,
          //                               prior)

          var predictionERP = makeUrOwnBackoffListener(utt, 
                                        // 1-priorParameters["mix"],
                                        wonkinessPrior,
                                        // wonkinessPrior[queryDictionary[measure]], 
                                        // speakerOptimality[queryDictionary[measure]], 
                                        speakerOptimality, 
                                        priorParameters["theta1"],
                                        priorParameters["theta2"]
                                        )

          // console.log('post predictionERP')
          var linkedERP = addGuessingNoise(marginalize(predictionERP, "world"), phi)

          // var uncooperativeERP = listener(utt, 
          //                               wonkinessPrior,
          //                               // wonkinessPrior[queryDictionary[measure]], 
          //                               // speakerOptimality[queryDictionary[measure]], 
          //                               speakerOptimality, 
          //                               0,
          //                               prior)

          // var mixedERP = cooperateAndNot(predictionERP, uncooperativeERP, cooperativeMix)


          // var stateERP = marginalize(predictionERP, "world")

          // var linkedERP = expectationPlusNoiseLink2(stateERP, sigma)
          // var linkedERP = ERPwithNoise




          var dataPoints = languageData[item][utterance]["comp_state"]
          var scr = modelScore(linkedERP, dataPoints)

          // console.log(item + utterance + scr)

          factor(scr)
          // foreach(linkedERP.support(), function(s){
          //     (utterance == "Some") ?
          //     query.add(["comp_state", item, utterance, s], Math.exp(linkedERP.score([], s))) : 
          //     null
          // })

          if (utterance != "short_filler") {
            query.add(["comp_state",item,utterance, "linkedExpectation"], expectation(linkedERP))
            // query.add(["comp_state",item,utterance, "stateExpectation"], expectation(stateERP))

            query.add(["comp_allprob",item,utterance, "linkedAllprob"] ,Math.exp(linkedERP.score([], 15)))
            // query.add(["comp_allprob",item,utterance, "stateAllprob"] ,Math.exp(stateERP.score([], 15)))

            var wonkyERP = marginalize(predictionERP, "wonky")

            query.add(["wonkiness",item,utterance, "wonkiness"] ,Math.exp(wonkyERP.score([], true)))
          }

          })

      // foreach(_.zip(_.range(0,16), prior),
      //   function(lst){
      //     query.add(["prior",item, lst[0],"prior"], lst[1])
      //   })


    })

        //   foreach(measures,
        //   function(measure){



        //   // var dataPoints = languageData[item][utterance][measure]
        //   var dataPoints = languageData[item][utterance][measure]


        //   // var linkedERP = marginalize(predictionERP, queryDictionary[measure])


        //     // modelScore(linkedERP, dataPoints)==-Infinity ? console.log('error'):null

        //     // don't condition on wonkiness data
        //    // var scr = modelScore(linkedERP, dataPoints)
        //   // var scr = ((measure=='wonkiness') | (measure=='comp_allprob') | (!(utterance=='Some'))) ? 
        //   var scr = ((measure=='wonkiness') | (measure=='comp_allprob')) ? 
        //                 0 : modelScore(linkedERP, dataPoints)
        //     // var scr = modelScore(linkedERP, dataPoints)
        //     // console.log(measure+" "+ utterance+" " +item +" "+ scr)
        //     // console.log(scr)
        //     factor(scr)

        //     // query.add([measure,item,utterance] ,expectation(linkedERP))

        //     // foreach(linkedERP.support(), function(s){
        //     //   query.add([measure, item, utterance, s], Math.exp(linkedERP.score([], s)))
        //     // })
        //     query.add([measure,item,utterance, "linkedExpectation"] ,expectation(linkedERP))


        // // if (measure=='comp_allprob') {
        // //   query.add(["allprob_scale","na","na"], scale)
        // //   query.add(["allprob_offset","na","na"], offset)
        // //   query.add(["allprob_sigma","na","na"], sigma)
        // //   // query.add(["worldly_speakerOptimality","na","na"], speakerOptimality["world"])
        // //   // query.add(["worldly_phi","na","na"], phi["world"])
        // //   // query.add(["worldly_wonkinessPrior","na","na"], wonkinessPrior["world"])
        // // }

        // // if (measure=='comp_state') {
        // //   query.add(["state_sigma","na","na", "na"], sigma)
        // // }


        //   })

        // query.add(["comp_state",item,utterance, "rawExpectation"] ,expectation(marginalize(predictionERP, "world")))
        // query.add(["comp_allprob",item,utterance, "rawExpectation"], Math.exp(marginalize(predictionERP, "world").score([], 15)))

        // })

      // query.add(["theta",item,"na", "na"], theta)

      // query.add(["phi",item,"na"], phi)

        // if (measure=='wonkiness') {
        //   // query.add(["wonky_scale","na","na"], scale)
        //   // query.add(["wonky_offset","na","na"], offset)
        //   // query.add(["wonky_softmax","na","na"], softmax)
        //   // query.add(["wonky_speakerOptimality","na","na"], speakerOptimality["wonky"])
        //   // query.add(["wonky_phi","na","na"], phi["wonky"])
        //   // query.add(["wonky_wonkinessPrior","na","na"], wonkinessPrior["wonky"])
      //   // }
      // query.add(["speakerOptimality",item,"na"],speakerOptimality)
      // query.add(["wonkinessPrior",item,"na"], wonkinessPrior)


  query.add(["wonkinessPrior","na","na","na"], wonkinessPrior)
  query.add(["speakerOptimality","na","na", "na"],speakerOptimality)
  // query.add(["speakerCooperativity","na","na", "na"],speakerCooperativity)
  query.add(["phi","na","na", "na"], phi)

  // query.add(["sigma","na","na", "na "], sigma)
  // query.add(["state_sigmaSome","na","na", "na"], sigma_some)
  // query.add(["state_sigmaAllNone","na","na", "na"], sigma_allnone)


   return query

}

// var utt = "Some"

// var prior = binomialMarbles(0.99)
// var speakerOptimality = 0.3
// var wonkinessPrior = 0.5
//   var predictionERP = listener(utt, 
//                                 wonkinessPrior,
//                                 // wonkinessPrior[queryDictionary[measure]], 
//                                 // speakerOptimality[queryDictionary[measure]], 
//                                 speakerOptimality, 
//                                 1,
//                                 prior)


// expectation(marginalize(predictionERP, "world"))

var mhiter = 20000
var burn = mhiter / 2

// var burn = 0
var results = IncrementalMH(dataAnalysis, mhiter, {"verbose":true, "burnin": burn})
// var results = HashMH(dataAnalysis, mhiter, {"verbose":true, "burnin": burn})

// var results = HashMH(dataAnalysis, mhiter, {"verbose":true})

console.log('model complete. writing data...')
// var outfile = 'results/wonkyFBTPosterior_justCompState_originalPriors_NowonkyTF_so_wp_nophi_allProb-sigma-scale-offset_CTS_incrMH'+ mhiter+'a.csv'

// var outfile = 'results/wonkyFBTPosterior_justCompState_wRepData_originalPriors_so_wp_NOphi_CTS_incrMH'+ mhiter+'burn'+mhiter/2+'a.csv'
// var outfile = 'results/wonkyFBTPosterior_justCompState_wRepData_originalPriors_so_wp_phiByItem_CTS_incrMH'+ mhiter+'burn'+burn+'a.csv'
// var outfile = 'results/wonkyFBTPosterior_fullPost_expecatationLink_justCompState_wRepData_originalPriors_so_wp_phi_CTS_incrMH'+ mhiter+'burn'+burn+'a.csv'
// var outfile = 'results/basicRSA_FBTPosterior_expecatationLink_sharedSigma_justCompState_wRepData_inclZero_originalPriors_so_wp_noPhi_CTS_hashMH'+ mhiter+'burn'+burn+'a.csv'
// var outfile = 'results/wonkyRSA_FBTPosterior_expecatationLink_sharedSigma_justCompState_wRepData_inclZero_4stepPriors_so_wp_noPhi_CTS_hashMH'+ mhiter+'burn'+burn+'a.csv'
// var outfile = 'results/wonkyRSA_fullPosterior_binomialBackoff_sansLink_justCompState_justSomeFillers_wRepData_inclZero_bdaPriors50kMAPphi_so_wp_noPhi_CTS_hashMH'+ mhiter+'burn'+burn+'a.csv'

// var outfile = 'results/wonkyRSA_fullPosterior_binomialBackoff_justCompState_wRepData_bdaPriors2thetas-removePhi50k_so_wp_phi_incrMH'+ mhiter+'burn'+burn+'a.csv'
// var outfile = 'results/wonkyRSA_fullPosterior_binomialBackoff_justCompState_wRepData_bdaPriors3thetas50k_so_wp_phi_incrMH'+ mhiter+'burn'+burn+'a.csv'
// var outfile = 'results/wonkyRSA_fullPosterior_unifBackoff_justCompState_wRepData_bdaPriors3thetas50k_so_wp_phi_incrMH'+ mhiter+'burn'+burn+'a.csv'
// var outfile = 'results/wonkyRSA_fullPosterior_unifBackoff_justCompState_wRepData_bdaPriors2thetas50k_so_wp_phi_incrMH'+ mhiter+'burn'+burn+'a.csv'
var outfile = 'results/wonkyRSA_fullPosterior_makeUrOwnBackoff_justCompState_wRepData_bdaPriors2thetas'+prioriter/1000+'k_so_wp_phi_incrMH'+ mhiter+'burn'+burn+'a.csv'

// var outfile = 'results/wonkyRSA_fullPosterior_unifBackoff_justCompState_wRepData_bdaPriors2thetas50kMAP_so_wp_phi_incrMH'+ mhiter+'burn'+burn+'a.csv'

// var outfile = 'results/wonkyFBTPosterior_justCompState_withRepData_originalPriors_so_wp_Nophi_CTS_allProb-sigma-scale-offset_incrMH'+ mhiter+'burn'+'0'+'a.csv'

wonkyutils.erpWriter(results, outfile)
console.log('wrote ' +outfile)


// languageData
// utterances

// languageData["sank marbles"]["long_filler"]["comp_state"]
// var theta = priorERPobject["were green bananas"]["val"]
// var prior = map(function(x){
//   return Math.exp(binomialERP.score([theta,15], x))
// }, _.range(0,16))


//   var predictionERP = listener("None", 
//                                 0.5,
//                                 // wonkinessPrior[queryDictionary[measure]], 
//                                 // speakerOptimality[queryDictionary[measure]], 
//                                 2, 
//                                 prior)

//   var stateERP = marginalize(predictionERP, "world")


//   stateERP


// priorERPobject["sank marbles"]["val"]
// console.log(theta)
// prior
// var item = items[2]
// var prior = df_prior["stuck to the wall baseballs"]//df_prior[items[2]]
// var prior = df_prior["melted ice cubes"]//df_prior[items[2]]
// var measure = "comp_state"
// var utterance= "Some"
// var wonkinessPrior = 0
// var speakerOptimality = 10
// var phi = 0
// var softmax = 1
// var sigma = 1.4
// var scale  = 1
// var offset = 0

// var predictionERP = listener(utterance, 
//                                         wonkinessPrior,
//                                         // wonkinessPrior[queryDictionary[measure]], 
//                                         // speakerOptimality[queryDictionary[measure]], 
//                                         speakerOptimality, 
//                                         prior)

// // // predictionERP

// var linkedERP = linkingFunction2(marginalize(predictionERP, queryDictionary[measure]), 
//                                     {"measure":measure, 
//                                    "sigma": sigma, 
//                                    "offset": offset, 
//                                     "scale": scale, 
//                                    // phi[queryDictionary[measure]],
//                                     "phi": phi,
//                                     "softmax": softmax})


// // marginalize(predictionERP, queryDictionary[measure])
// expectation(linkedERP)

// var discreteGaussian2 = function(mu, sigma, bins){
//   // return map(function(d){
//   //     return Math.exp(gaussianERP.score([wonkyutils.logit(mu), sigma], 
//   //                                      wonkyutils.logit(d)))
//   // }, bins)
//   return wonkyutils.logit(mu)
// }


// var expectationPlusNoiseLink2 = function(marginalERP, sigma){
//   var states = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
//   // var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
//   var bins = [ 0.01,
//   0.066,
//   0.133,
//   0.2,
//   0.266,
//   0.333,
//   0.4,
//   0.466,
//   0.533,
//   0.6,
//   0.666,
//   0.733,
//   0.8,
//   0.866,
//   0.933,
//   0.99 ]
//   var expvalNormalized = expectation(marginalERP) / states[states.length-1]
//   var noisyExpectationProbs = discreteGaussian2(expvalNormalized, sigma, bins)
//   // return states[discrete(noisyExpectationProbs)]
//   return noisyExpectationProbs
// }



// // linkedERP
// // // predictionERP

// // expectationPlusNoiseLink2(marginalize(predictionERP, queryDictionary[measure]), 1)
// var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
//   var states = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]

// var stateProbs = map(function(x){return x / states[states.length - 1]},states )

// stateProbs
// // map(function(x){return wonkyutils.logit(x)}, bins)
// expectation(marginalize(predictionERP, queryDictionary[measure]))
// // prior
//   var states = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]

// sum(map(function(x){return x[0]*x[1]},
//   _.zip(prior, states)
//   ))

// prior
// predictionERP


// var logitNoiseLogistic2 = function(prob, sigma, offset, scale, bins){
//   var discreteGaussian=  
//   return 
//   return discreteGaussian
// }





// expectationPlusNoiseLink(marginalize(predictionERP, queryDictionary[measure]), {sigma: 2})

// expval / states[states.length-1]
// 
// linkedERP



// var i = 'fell down card towers'
// var u = "Some"
// var m = "comp_state"

// var wonkinessPrior = 0.5
// var speakerOptimality = 2

//   var predictionERP = marginalize(listener(u, 
//                                           wonkinessPrior,
//                                           speakerOptimality, 
//                                           df_prior[i]), "world")


// Math.exp(modelScore(predictionERP, languageData[i][u][m]))





// expectation(predictionERP)

    // predictionERP
      // var dataPoints = languageData[item][utterance]["comp_state"]

//     return MAP(predictionERP)


// _.object(map(function(i){

//   return [i, sum(languageData[i]["Some"]["comp_state"]) / languageData[i]["Some"]["comp_state"].length]

// }, items)


// )
// languageData
// var m = (languageData[1][3]!="15" & languageData[1][2]=='Some')
// m

// _.filter(languageData['stuck to the wall baseballs']["Some"]["comp_state"],
//   function(x){return x > 0})
// _.keys(empiricalPriors).length


// empiricalPriors



// var empiricalPriors = { 'laughed lawyers': 
//    [ 0.0059509,
//      0.0114966,
//      0.0222103,
//      0.0429082,
//      0.0828949,
//      0.0988491,
//      0.0979429,
//      0.1104172,
//      0.1418795,
//      0.1145286,
//      0.108014,
//      0.0581106,
//      0.0343299,
//      0.0259816,
//      0.0293129,
//      0.015173 ],
//   'melted ice cubes': 
//    [ 1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      7e-7,
//      0.0000276,
//      0.0012921,
//      0.0606927,
//      0.9379858 ],
//   'stuck to the wall baseballs': 

//      [ 0.6234761,
//      0.2090756,
//      0.1013102,
//      0.0160822,
//      0.0076818,
//      0.0359784,
//      0.0054297,
//      0.0008195,
//      0.0001238,
//      0.0000188,
//      0.0000029,
//      5e-7,
//      2e-7,
//      1e-7,
//      1e-7,
//      1e-7 ]}
 // [ 1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     7e-7,
 //     0.0000276,
 //     0.0012921,
 //     0.0606927,
 //     0.9379858 ].reverse()}


// map(function(speakerOptimality){
//   return map(function(item){
//     var utterance = "Some"
//     var wonkinessPrior = 0.5
//     var predictionERP = listener(utterance, 
//                                             wonkinessPrior,
//                                             speakerOptimality, 
//                                             item, 
//                                             "world")
//       var dataPoints = languageData[item][utterance]["comp_state"]

//     return MAP(predictionERP)



//     }, _.keys(empiricalPriors))


// },[1,2,3,4,5])



// map(function(item){
//     var utterance = "Some"
//     var wonkinessPrior = 0.5
//     var speakerOptimality = 2
//     var predictionERP = listener(utterance, 
//                                             wonkinessPrior,
//                                             speakerOptimality, 
//                                             item, 
//                                             "world")

//     // return map(function(x){return Math.exp(predictionERP.score([],x))}, predictionERP.support())



//       return     sum(map(function(x){return predictionERP.score([],x)}, languageData[item][utterance]["comp_state"]))




//     }, _.keys(empiricalPriors))




// var utterance = "Some"
// var wonkinessPrior = 0.5
// var speakerOptimality = 2
// var predictionERP = listener(utterance, 
//                                         wonkinessPrior,
//                                         speakerOptimality, 
//                                         "stuck to the wall baseballs", 
//                                         "wonky")
// Math.exp(predictionERP.score([], true))
