// webppl wonky-tfbt.wppl --require-js ./wonkyutils 

// helper functions

var shape_alpha = function(gamma,delta){return gamma * delta}
var shape_beta = function(gamma,delta){return (1-gamma) * delta}

var transpose = function(lst){
  return _.zip.apply(_, lst)
}


var modelScore = function(modelERP, data){
    return sum(
      map(
        function(d){return modelERP.score([],d)},
      data)
      )
  }


// data
var empiricalPriors = wonkyutils.readPriors()
var listOf90Items = _.keys(empiricalPriors)

var languageData = wonkyutils.readData()

var dependentMeasures = ['comp_state','comp_allprob','wonkiness']
var quantifiers = ["Some","All","None"]


var numObjs = 15

var worldPrior = function() {
  var marbles = randomInteger(numObjs+1) //15 marbles... 0-15 can sink
  return marbles
}

var utterancePrior = function() {
  var utterances = ["Some",
                    "All",
                    "None"]
  return uniformDraw(utterances)
}

var meaning = function(utt,world) {
  return utt=="Some"? world>0 :
  utt=="All"? world==numObjs :
  utt=="None"? world==0 :
  true
}

var literalListener = cache(function(utterance, prior) {
                            Enumerate(function(){
                                      var world = discrete(prior)
                                      var m = meaning(utterance, world)
                                      factor(m?0:-Infinity)
                                      return world
                                      })
                            })

var speaker = cache(function(world, prior) {
                    Enumerate(function(){
                              var utterance = utterancePrior()
                              var L = literalListener(utterance, prior)
                              factor(L.score([],world))
                              return utterance
                              })
                    })

var listener = cache(function(utterance,wonkinessPrior,speakerOptimality,item,query) {
                      Enumerate(function(){
                                var wonky = flip(wonkinessPrior)
                                var prior = wonky ? 
                                      wonkyutils.fillArray(1,16) :
                                      empiricalPriors[item]
                                    // [.1,.1,.1,.1,.1,.1,.1,.1,.1,.1,.1,.1,.1,.1,1,5]
                                    //empiricalPriors[item]
                                var world = discrete(prior)
                                var S = speaker(world, prior)
                                factor(speakerOptimality*S.score([],utterance))
                                var queryStatement = {"world":world,
                                                      "wonky":wonky}
                                return queryStatement[query]
                                })
                    })


var getMarginal = function(myERP, measure){
  Enumerate(function(){
    var x = sample(myERP)
    return x[measure]
  })
}

var linkingFunction = function(marginalERP, measure, linkingBetaConcentration, wonkySoftmax){
  Enumerate(function(){

    // var queryStatement = sample(betaERP, [shape_alpha(posteriorProb, delta),
    //                                        shape_beta(posteriorProb, delta)])

    var discreteModelPredSliderVal = function(gamma, delta) {
      var bins = [0.001,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.999]
      var discretizeBeta = function(gamma, delta){
        var shape_alpha = gamma * delta
        var shape_beta = (1-gamma) * delta
        var betaPDF = function(x){
          return Math.pow(x,shape_alpha-1)*
              Math.pow((1-x),shape_beta-1)
        }
        return map(betaPDF, bins)
      }
      var quintileProbs = discretizeBeta(gamma,delta)
      //return quintileProbs
       var sliderVal = bins[discrete(quintileProbs)]
       return sliderVal
    }


    var posteriorProb = measure == "comp_allprob" ? 
                                Math.exp(marginalERP.score([], 15)):
                                wonkyutils.softmax(Math.exp(marginalERP.score([], true)),wonkySoftmax)

    var queryStatement = discreteModelPredSliderVal(posteriorProb,linkingBetaConcentration)

    return queryStatement
 //   return [posteriorProb, linkingBetaConcentration]
}
)}




var queryDictionary = {'comp_state':"world",
                        'comp_allprob':"world",
                        'wonkiness':"wonky"}


var dataAnalysis = function(){

  // Priors
  // var speakerOptimality = uniform(0,10)

 var speakerOptimality = 0.1+(randomInteger(20)/4)
  // var speakerOptimality = {
  //                       'comp_state':0.1+(randomInteger(20)/4),
  //                       'comp_allprob':0.1+(randomInteger(20)/4),
  //                       'wonkiness':0.1+(randomInteger(20)/4)
  // }

   // var wonkinessPrior = uniform(0,1)
  var wonkinessPrior = 0.01+(randomInteger(10)/10)
  var wonkySoftmax = 0.01+randomInteger(20)/4
 // var linkingBetaConcentration = uniform(0,10)
   var linkingBetaConcentration = (randomInteger(20)+1)/3


  var posteriorPredictive = 
    map(function(item){
   //   console.log(item)
      return map(function(utterance){
     //   console.log(utterance)

        return map(function(measure){
   //       console.log(measure)
          var dataPoints = languageData[item][utterance][measure]
          var query = queryDictionary[measure]

          // for model with just 1 speakerOptimality for all 3 tasks
          var predictionERP = listener(utterance, wonkinessPrior, speakerOptimality, item, query)
          // for speakerOptimalities that differ across tasks
          // var predictionERP = listener(utterance, wonkinessPrior, speakerOptimality[measure], item, query)


          // if comprehension task "give a number", use the marginal posterior directly
          // otherwise, need to connect to the slider value somehow
          // here, we assume the slider value is the mean of a beta (possibly discretized, for implementation)
          // and there is some concentration (delta) variable; this is a mere nuisance, data-analysis parameter
          var linkedERP = measure == "comp_state" ?
                            predictionERP :
                            linkingFunction(predictionERP, measure, linkingBetaConcentration, wonkySoftmax)

         // modelScore(linkedERP, dataPoints)==-Infinity ? console.log('error'):null
         factor(modelScore(linkedERP, dataPoints))
          
          // get posterior prob for each of the values in the support
           return map(
            function(val){
              return [measure, item, utterance, val, Math.exp(linkedERP.score([], val))]
            },linkedERP.support())

          }, dependentMeasures)

        }, quantifiers)

      }, listOf90Items)


//  return [speakerOptimality,wonkinessPrior,linkingBetaConcentration]
 // return [_.values(speakerOptimality),wonkinessPrior,linkingBetaConcentration,
 //        _.flatten(_.flatten(_.flatten(posteriorPredictive, true),true),true)]
 return [speakerOptimality,wonkinessPrior,linkingBetaConcentration,wonkySoftmax,
        _.flatten(_.flatten(_.flatten(posteriorPredictive, true),true),true)]



 // return _.flatten(_.flatten(_.flatten(posteriorPredictive, true),true),true)
}

// dataAnalysis()

var mhsamples = 10000
var burn = 1000
var results = MH(dataAnalysis,mhsamples, burn)
// //var results = Enumerate(dataAnalysis, 10)

// results

console.log('model complete. munging data...')

// var marginalize = function(myERP, index){
//   Enumerate(function(){
//     var x = sample(myERP)
//     return x[index]
//   })
// }


var output = wonkyutils.writeERP(results)
//output.unshift([_.keys(queryDictionary).join('so,')+',wonkinessPrior,linkingBetaConcentration,posteriorPredictive','posteriorProb']);
//output.unshift(['speakerOptimality,wonkinessPrior,linkingBetaConcentration,posteriorPredictive','posteriorProb']);
output.unshift(['speakerOptimality,wonkinessPrior,linkingBetaConcentration,softmaxWonky,posteriorPredictive','posteriorProb']);

var outfile = 'results/wonky_tfbtPosterior_predictiveAnddiscreteParams_softmaxWonky_mh'+ mhsamples+ 'b'+burn+'.csv'
//  'results/wonky_tfbtPosterior_predictiveAnddiscreteParams_3speakerOptimalities_mh'+ mhsamples+ 'b'+burn+'.csv')


wonkyutils.writeCSV(output, outfile)

console.log('wrote ' +outfile)

// var speakerOptPosterior = writeERP(marginalize(results,0))
// speakerOptPosterior.unshift(['speakerOptimality','posteriorProb']);

// var wonkinessPriorPosterior = writeERP(marginalize(results,1))
// wonkinessPriorPosterior.unshift(['wonkinessPrior','posteriorProb']);

// var linkingBetaPosterior = writeERP(marginalize(results,2))
// linkingBetaPosterior.unshift(['linkingBetaConcentration','posteriorProb']);


// wonkyutils.writeCSV(speakerOptPosterior, 
// // 'wonky_tfbtPosterior_speakerOptimality_enumerateCoarse'+ mhsamples+ '.csv')
//   'results/wonky_tfbtPosterior_speakerOptimalityC_mh'+ mhsamples+ 'b'+burn+'.csv')

// wonkyutils.writeCSV(wonkinessPriorPosterior, 
// //  'wonky_tfbtPosterior_wonkinessPrior_enumerateCoarse'+ mhsamples+ '.csv')
//   'results/wonky_tfbtPosterior_wonkinessPriorC_mh'+ mhsamples+ 'b'+burn+'.csv')

// wonkyutils.writeCSV(linkingBetaPosterior, 
// //  'wonky_tfbtPosterior_linkingBetaConcentration_enumerateCoarse'+ mhsamples+ '.csv')
//   'results/wonky_tfbtPosterior_linkingBetaConcentrationC_mh'+ mhsamples+ 'b'+burn+'.csv')

