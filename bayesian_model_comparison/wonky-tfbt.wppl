// time webppl wonky-tfbt.wppl --require wonkyutils


// helper functions

var foreach = function(lst, fn) {
    var foreach_ = function(i) {
        if (i < lst.length) {
            fn(lst[i]);
            foreach_(i + 1);
        }
    };
    foreach_(0);
};


// use first row as (header) variable names
var dataFrame = function(rawCSV){
    return map(function(row){
        return _.object(_.zip(rawCSV[0],row))
    }, rawCSV.slice(1))
}

var marginalize = function(myERP, index){
  Enumerate(function(){
    var x = sample(myERP)
    return x[index]
  })
}


var modelScore = function(modelERP, data){
    return reduce(function(dataPoint, memo) {
                return memo + modelERP.score([], dataPoint);
            }, 0, data)
  }


// data

// var linkingFunction = function(marginalERP, measure, linkingBetaConcentration, wonkySoftmax){
// // var linkingFunction = cache(function(marginalERP, measure, sigma, wonkySoftmax){
//   Enumerate(function(){

//     // var queryStatement = sample(betaERP, [shape_alpha(posteriorProb, delta),
//     //                                        shape_beta(posteriorProb, delta)])

//     var discreteModelPredSliderVal = function(gamma, delta) {
//       var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
//       var discretizeBeta = function(gamma, delta){
//         var shape_alpha = gamma * delta
//         var shape_beta = (1-gamma) * delta
//         var betaPDF = function(x){
//           return Math.pow(x,shape_alpha-1)*
//               Math.pow((1-x),shape_beta-1)
//         }
//         return map(betaPDF, bins)
//       }
//       var quintileProbs = discretizeBeta(gamma,delta)
//       //return quintileProbs
//        var sliderVal = bins[discrete(quintileProbs)]
//        return sliderVal
//     }

var logitNoiseLogistic = function(prob, sigma, offset, scale, bins){
  var discreteGaussian=  map(function(d){
    return Math.exp(gaussianERP.score([wonkyutils.logit(prob), sigma], 
                                       wonkyutils.logit(d)))
  }, bins)
  var noisyLogit_p =  wonkyutils.logit(bins[discrete(discreteGaussian)])
  var logisitic_noisy_p = wonkyutils.logistic(noisyLogit_p, offset, scale)

 return wonkyutils.roundToBin(logisitic_noisy_p, bins)
}

var queryDictionary = {'comp_state':"world",
                        'comp_allprob':"world",
                        'wonkiness':"wonky"}



var discreteGaussian = function(mu, sigma, bins){
  return map(function(d){
      return Math.exp(gaussianERP.score([wonkyutils.logit(mu), sigma], 
                                       wonkyutils.logit(d)))
  }, bins)
}


var expectationPlusNoiseLink = function(marginalERP, sigma){
  var states = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
  var bins = [ 0.01,
  0.066,
  0.133,
  0.2,
  0.266,
  0.333,
  0.4,
  0.466,
  0.533,
  0.6,
  0.666,
  0.733,
  0.8,
  0.866,
  0.933,
  0.99 ]
  var expvalNormalized = expectation(marginalERP) / states[states.length-1]

  var expvalAvoidEnds = (expvalNormalized>0.99) ? 0.99 :
                                        (expvalNormalized < 0.01) ? 0.01 :
                                              expvalNormalized
  var noisyExpectationProbs = discreteGaussian(expvalAvoidEnds, sigma, bins)
  return states[discrete(noisyExpectationProbs)]
  // return noisyExpectationProbs
}




var guessingBehavior = function(measure, bins, states){
  return  (measure == "comp_allprob") ?  uniformDraw(bins) : 
          (measure == "comp_state") ?  uniformDraw(states) :
                                      flip(0.5) ? 1: 0 
}



var linkingFunction2 =  function(marginalERP, options){

  Enumerate(function(){

    var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
    var states = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
    var posteriorProb = options.measure == "comp_allprob" ? 
                                Math.exp(marginalERP.score([], 15)):
                               wonkyutils.softmax(Math.exp(marginalERP.score([], true)),options.softmax)
                               // Math.exp(marginalERP.score([], true))

    var posteriorProbAvoidEnds = (posteriorProb>0.99) ? 0.99 :
                                        (posteriorProb < 0.01) ? 0.01 :
                                              posteriorProb

    var queryStatement =  flip(options.phi) ?  guessingBehavior(options.measure, bins, states) :
                                    (options.measure == "comp_allprob") ? 
                                                    // wonkyutils.roundToBin(posteriorProbAvoidEnds, bins) :
                                                  // wonkyutils.roundToBin(wonkyutils.logitLogistic(posteriorProbAvoidEnds, offset, scale), bins) :
                                                   logitNoiseLogistic(posteriorProbAvoidEnds, options.sigma, options.offset, options.scale, bins) :
                                                   // posteriorProbAvoidEnds : 
                                     (options.measure == "comp_state") ? 
                                                    expectationPlusNoiseLink(marginalERP, options.sigma) : 
                                                    // logitNoiseLogistic(posteriorProbAvoidEnds,wonkySigma, wonkyOffset, wonkyScale)
                                                    flip(posteriorProb) ? 1 : 0
                                                    // flip(wonkyutils.logitLogistic(posteriorProb, options.offset, options.scale))?1:0


    return queryStatement
})
}




var dfilepath = 'data/';
// var datafile = wonkyutils.readCSV(dfilepath+ 'empirical_binarywonky_allQ.csv').data
// var df_language = dataFrame(datafile.slice(0, datafile.length-1))
var languageData = wonkyutils.readData()

// var priorfile = wonkyutils.readCSV(dfilepath+ 'priors.csv').data
// // var priorfile = wonkyutils.readCSV(dfilepath+ 'priors_original.csv').data
// var dPriors = priorfile.slice(0,priorfile.length)
// var df_prior = _.object(map(function(lst){return [lst[0], map(function(x){return wonkyutils.wpParseFloat(x)}, lst.slice(1))]}, dPriors))

// var measures = _.uniq(_.pluck(df_language, "measure"))
// var items = _.uniq(_.pluck(df_language, "item"))
// var utterances = _.uniq(_.pluck(df_language, "utterance"))

var items = _.keys(languageData)

// var measures = _.uniq(_.pluck(df_language, "measure"))
var utterances = _.keys(languageData[items[0]])

var measures = _.keys(languageData[items[0]][utterances[0]])
// var measures = ["comp_state", "comp_allprob"]

var measures = ["comp_state"]
// var utterances = ["Some"]

// var items = ['fell down card towers']
// var guessingLink =  function(marginalERP, phi){
//   Enumerate(function(){
//     var states = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
//     var queryStatement = flip(phi) ? uniformDraw(states) : sample(marginalERP)
//     return queryStatement
// })
// }

var prioriter = 50000
var priorburn = prioriter / 2

var priorERP = IncrementalMH(priorModel, prioriter, {verbose:true, burnin: priorburn})

var priorERPobject = _.object(map(function(i){
  return [i, marginalizeERP(priorERP, i)]
}, items))


var dataAnalysis = function(){



      // Priors (continuous)
      var speakerOptimality = uniform(0,10)
      // var speakerOptimality = {
      //   "world":uniform(0,10),
      //   "wonky":uniform(0,10)
      // }


      // var wonkinessPrior = uniform(0,1)
      var wonkinessPrior = 0
      // var wonkinessPrior = {
      //   "world": uniform(0,1),
      //   "wonky": uniform(0,1)
      // }
      var phi = 0
      // var phi = uniform(0,1)
      // var phi = 0.0001

      // var phi = {
      //   "world":uniform(0,1),
      //   "wonky":uniform(0,1)
      // }

      var sigma = uniform(0.1,3)
      // var sigma = 0.3
      // var sigma_some = uniform(0.1,2)
      // var sigma_allnone = uniform(0.1,2)

      // console.log(measure)
      // var speakerOptimality = 

     // var sigma = measure=='comp_allprob' ? uniform(0.1,5) : 
     //             measure=='comp_state' ? uniform(0.1,2) :
     //                                     0


     // var sigma = 0
     // var softmax = measure=="wonkiness" ? uniform(0,10) : 1
     var softmax = 1;
      // var linkingSigma = 0
      // var scale = measure=='comp_allprob' ? uniform(0,4) : 1
      var scale = 1
      // var offset = measure=='comp_allprob' ? sample(gaussianDriftERP,[0,10]) : 0
      var offset = 0
      // var scale = 1
      // var offset = 0




  foreach(items,
    function(item){

      // var phi = uniform(0,1)

      // var prior = df_prior[item]
      var theta = sample(priorERPobject[item])
      var prior = map(function(x){
        return Math.exp(binomialERP.score([theta,15], x))
      }, _.range(0,16))


      // console.log(item)
    foreach(utterances,
      function(utterance){

          // var sigma = utterance=="Some" ? sigma_some : sigma_allnone


          var predictionERP = listener(utterance, 
                                        wonkinessPrior,
                                        // wonkinessPrior[queryDictionary[measure]], 
                                        // speakerOptimality[queryDictionary[measure]], 
                                        speakerOptimality, 
                                        prior)

          foreach(measures,
          function(measure){



          // var dataPoints = languageData[item][utterance][measure]
          var dataPoints = languageData[item][utterance][measure]

          var linkedERP = linkingFunction2(marginalize(predictionERP, queryDictionary[measure]), 
                                              {"measure":measure, 
                                               "sigma": sigma, 
                                               "offset": offset, 
                                                "scale": scale, 
                                               // phi[queryDictionary[measure]],
                                                "phi": phi,
                                                "softmax": softmax})

          // var linkedERP = marginalize(predictionERP, queryDictionary[measure])


            // modelScore(linkedERP, dataPoints)==-Infinity ? console.log('error'):null

            // don't condition on wonkiness data
           // var scr = modelScore(linkedERP, dataPoints)
          // var scr = ((measure=='wonkiness') | (measure=='comp_allprob') | (!(utterance=='Some'))) ? 
          var scr = ((measure=='wonkiness') | (measure=='comp_allprob')) ? 
                        0 : modelScore(linkedERP, dataPoints)
            // var scr = modelScore(linkedERP, dataPoints)
            // console.log(measure+" "+ utterance+" " +item +" "+ scr)
            // console.log(scr)
            factor(scr)

            // query.add([measure,item,utterance] ,expectation(linkedERP))

            // foreach(linkedERP.support(), function(s){
            //   query.add([measure, item, utterance, s], Math.exp(linkedERP.score([], s)))
            // })
            query.add([measure,item,utterance, "linkedExpectation"] ,expectation(linkedERP))


        // if (measure=='comp_allprob') {
        //   query.add(["allprob_scale","na","na"], scale)
        //   query.add(["allprob_offset","na","na"], offset)
        //   query.add(["allprob_sigma","na","na"], sigma)
        //   // query.add(["worldly_speakerOptimality","na","na"], speakerOptimality["world"])
        //   // query.add(["worldly_phi","na","na"], phi["world"])
        //   // query.add(["worldly_wonkinessPrior","na","na"], wonkinessPrior["world"])
        // }

        // if (measure=='comp_state') {
        //   query.add(["state_sigma","na","na", "na"], sigma)
        // }


          })

        query.add(["comp_state",item,utterance, "rawExpectation"] ,expectation(marginalize(predictionERP, "world")))
        query.add(["comp_allprob",item,utterance, "rawExpectation"],
                Math.exp(marginalize(predictionERP, "world").score([], 15)))

         var linkedAllProb = linkingFunction2(marginalize(predictionERP, "world"), 
                                              {"measure":"comp_allprob", 
                                               "sigma": sigma, 
                                               "offset": offset, 
                                                "scale": scale, 
                                               // phi[queryDictionary[measure]],
                                                "phi": phi,
                                                "softmax": softmax})

        query.add(["comp_allprob",item,utterance, "linkedExpectation"], expectation(linkedAllProb))
        query.add(["wonkiness",item,utterance, "rawExpectation"],
                Math.exp(marginalize(predictionERP, "wonky").score([], true)))

        })

      query.add(["theta",item,"na", "na"], theta)

      // query.add(["phi",item,"na"], phi)

        // if (measure=='wonkiness') {
        //   // query.add(["wonky_scale","na","na"], scale)
        //   // query.add(["wonky_offset","na","na"], offset)
        //   // query.add(["wonky_softmax","na","na"], softmax)
        //   // query.add(["wonky_speakerOptimality","na","na"], speakerOptimality["wonky"])
        //   // query.add(["wonky_phi","na","na"], phi["wonky"])
        //   // query.add(["wonky_wonkinessPrior","na","na"], wonkinessPrior["wonky"])
      //   // }
      // query.add(["speakerOptimality",item,"na"],speakerOptimality)
      // query.add(["wonkinessPrior",item,"na"], wonkinessPrior)

      })

  query.add(["wonkinessPrior","na","na","na"], wonkinessPrior)
  query.add(["speakerOptimality","na","na", "na"],speakerOptimality)
  // query.add(["phi","na","na", "na"], phi)
  query.add(["sigma","na","na", "na"], sigma)
  // query.add(["state_sigmaSome","na","na", "na"], sigma_some)
  // query.add(["state_sigmaAllNone","na","na", "na"], sigma_allnone)


   return query

}


var mhiter = 10000
var burn = mhiter / 2

// var burn = 0
var results = IncrementalMH(dataAnalysis, mhiter, {"verbose":true, "burnin": burn})
// var results = HashMH(dataAnalysis, mhiter, {"verbose":true})

console.log('model complete. writing data...')
// var outfile = 'results/wonkyFBTPosterior_justCompState_originalPriors_NowonkyTF_so_wp_nophi_allProb-sigma-scale-offset_CTS_incrMH'+ mhiter+'a.csv'

// var outfile = 'results/wonkyFBTPosterior_justCompState_wRepData_originalPriors_so_wp_NOphi_CTS_incrMH'+ mhiter+'burn'+mhiter/2+'a.csv'
// var outfile = 'results/wonkyFBTPosterior_justCompState_wRepData_originalPriors_so_wp_phiByItem_CTS_incrMH'+ mhiter+'burn'+burn+'a.csv'
// var outfile = 'results/wonkyFBTPosterior_fullPost_expecatationLink_justCompState_wRepData_originalPriors_so_wp_phi_CTS_incrMH'+ mhiter+'burn'+burn+'a.csv'
// var outfile = 'results/basicRSA_FBTPosterior_expecatationLink_sharedSigma_justCompState_wRepData_inclZero_originalPriors_so_wp_noPhi_CTS_hashMH'+ mhiter+'burn'+burn+'a.csv'
// var outfile = 'results/wonkyRSA_FBTPosterior_expecatationLink_sharedSigma_justCompState_wRepData_inclZero_4stepPriors_so_wp_noPhi_CTS_hashMH'+ mhiter+'burn'+burn+'a.csv'
var outfile = 'results/regularRSA_FBTPosterior_expecatationLink_sharedSigma_justCompState_wRepData_inclZero_bdaPriors50k_so_wp_noPhi_CTS_incrMH'+ mhiter+'burn'+burn+'a.csv'

// var outfile = 'results/wonkyFBTPosterior_justCompState_withRepData_originalPriors_so_wp_Nophi_CTS_allProb-sigma-scale-offset_incrMH'+ mhiter+'burn'+'0'+'a.csv'

wonkyutils.erpWriter(results, outfile)
console.log('wrote ' +outfile)


// var theta = sample(priorERPobject["sank marbles"])
// console.log(theta)
// prior
// var item = items[2]
// var prior = df_prior["stuck to the wall baseballs"]//df_prior[items[2]]
// var prior = df_prior["melted ice cubes"]//df_prior[items[2]]
// var measure = "comp_state"
// var utterance= "Some"
// var wonkinessPrior = 0
// var speakerOptimality = 10
// var phi = 0
// var softmax = 1
// var sigma = 1.4
// var scale  = 1
// var offset = 0

// var predictionERP = listener(utterance, 
//                                         wonkinessPrior,
//                                         // wonkinessPrior[queryDictionary[measure]], 
//                                         // speakerOptimality[queryDictionary[measure]], 
//                                         speakerOptimality, 
//                                         prior)

// // // predictionERP

// var linkedERP = linkingFunction2(marginalize(predictionERP, queryDictionary[measure]), 
//                                     {"measure":measure, 
//                                    "sigma": sigma, 
//                                    "offset": offset, 
//                                     "scale": scale, 
//                                    // phi[queryDictionary[measure]],
//                                     "phi": phi,
//                                     "softmax": softmax})


// // marginalize(predictionERP, queryDictionary[measure])
// expectation(linkedERP)

// var discreteGaussian2 = function(mu, sigma, bins){
//   // return map(function(d){
//   //     return Math.exp(gaussianERP.score([wonkyutils.logit(mu), sigma], 
//   //                                      wonkyutils.logit(d)))
//   // }, bins)
//   return wonkyutils.logit(mu)
// }


// var expectationPlusNoiseLink2 = function(marginalERP, sigma){
//   var states = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
//   // var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
//   var bins = [ 0.01,
//   0.066,
//   0.133,
//   0.2,
//   0.266,
//   0.333,
//   0.4,
//   0.466,
//   0.533,
//   0.6,
//   0.666,
//   0.733,
//   0.8,
//   0.866,
//   0.933,
//   0.99 ]
//   var expvalNormalized = expectation(marginalERP) / states[states.length-1]
//   var noisyExpectationProbs = discreteGaussian2(expvalNormalized, sigma, bins)
//   // return states[discrete(noisyExpectationProbs)]
//   return noisyExpectationProbs
// }



// // linkedERP
// // // predictionERP

// // expectationPlusNoiseLink2(marginalize(predictionERP, queryDictionary[measure]), 1)
// var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
//   var states = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]

// var stateProbs = map(function(x){return x / states[states.length - 1]},states )

// stateProbs
// // map(function(x){return wonkyutils.logit(x)}, bins)
// expectation(marginalize(predictionERP, queryDictionary[measure]))
// // prior
//   var states = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]

// sum(map(function(x){return x[0]*x[1]},
//   _.zip(prior, states)
//   ))

// prior
// predictionERP


// var logitNoiseLogistic2 = function(prob, sigma, offset, scale, bins){
//   var discreteGaussian=  
//   return 
//   return discreteGaussian
// }





// expectationPlusNoiseLink(marginalize(predictionERP, queryDictionary[measure]), {sigma: 2})

// expval / states[states.length-1]
// 
// linkedERP



// var i = 'fell down card towers'
// var u = "Some"
// var m = "comp_state"

// var wonkinessPrior = 0.5
// var speakerOptimality = 2

//   var predictionERP = marginalize(listener(u, 
//                                           wonkinessPrior,
//                                           speakerOptimality, 
//                                           df_prior[i]), "world")


// Math.exp(modelScore(predictionERP, languageData[i][u][m]))





// expectation(predictionERP)

    // predictionERP
      // var dataPoints = languageData[item][utterance]["comp_state"]

//     return MAP(predictionERP)


// _.object(map(function(i){

//   return [i, sum(languageData[i]["Some"]["comp_state"]) / languageData[i]["Some"]["comp_state"].length]

// }, items)


// )
// languageData
// var m = (languageData[1][3]!="15" & languageData[1][2]=='Some')
// m

// _.filter(languageData['stuck to the wall baseballs']["Some"]["comp_state"],
//   function(x){return x > 0})
// _.keys(empiricalPriors).length


// empiricalPriors



// var empiricalPriors = { 'laughed lawyers': 
//    [ 0.0059509,
//      0.0114966,
//      0.0222103,
//      0.0429082,
//      0.0828949,
//      0.0988491,
//      0.0979429,
//      0.1104172,
//      0.1418795,
//      0.1145286,
//      0.108014,
//      0.0581106,
//      0.0343299,
//      0.0259816,
//      0.0293129,
//      0.015173 ],
//   'melted ice cubes': 
//    [ 1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      1e-7,
//      7e-7,
//      0.0000276,
//      0.0012921,
//      0.0606927,
//      0.9379858 ],
//   'stuck to the wall baseballs': 

//      [ 0.6234761,
//      0.2090756,
//      0.1013102,
//      0.0160822,
//      0.0076818,
//      0.0359784,
//      0.0054297,
//      0.0008195,
//      0.0001238,
//      0.0000188,
//      0.0000029,
//      5e-7,
//      2e-7,
//      1e-7,
//      1e-7,
//      1e-7 ]}
 // [ 1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     1e-7,
 //     7e-7,
 //     0.0000276,
 //     0.0012921,
 //     0.0606927,
 //     0.9379858 ].reverse()}


// map(function(speakerOptimality){
//   return map(function(item){
//     var utterance = "Some"
//     var wonkinessPrior = 0.5
//     var predictionERP = listener(utterance, 
//                                             wonkinessPrior,
//                                             speakerOptimality, 
//                                             item, 
//                                             "world")
//       var dataPoints = languageData[item][utterance]["comp_state"]

//     return MAP(predictionERP)



//     }, _.keys(empiricalPriors))


// },[1,2,3,4,5])



// map(function(item){
//     var utterance = "Some"
//     var wonkinessPrior = 0.5
//     var speakerOptimality = 2
//     var predictionERP = listener(utterance, 
//                                             wonkinessPrior,
//                                             speakerOptimality, 
//                                             item, 
//                                             "world")

//     // return map(function(x){return Math.exp(predictionERP.score([],x))}, predictionERP.support())



//       return     sum(map(function(x){return predictionERP.score([],x)}, languageData[item][utterance]["comp_state"]))




//     }, _.keys(empiricalPriors))




// var utterance = "Some"
// var wonkinessPrior = 0.5
// var speakerOptimality = 2
// var predictionERP = listener(utterance, 
//                                         wonkinessPrior,
//                                         speakerOptimality, 
//                                         "stuck to the wall baseballs", 
//                                         "wonky")
// Math.exp(predictionERP.score([], true))
