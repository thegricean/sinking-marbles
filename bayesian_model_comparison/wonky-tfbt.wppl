// webppl wonky-tfbt.wppl --require-js ./wonkyutils --require-wppl wonkyRSA.wppl

// helper functions

var shape_alpha = function(gamma,delta){return gamma * delta}
var shape_beta = function(gamma,delta){return (1-gamma) * delta}

var transpose = function(lst){
  return _.zip.apply(_, lst)
}


var modelScore = function(modelERP, data){
    return reduce(function(dataPoint, memo) {
                return memo + modelERP.score([], dataPoint);
            }, 0, data)
  }


// data
var empiricalPriors = wonkyutils.readPriors()
var languageData = wonkyutils.readData()

var listOf90Items = _.keys(empiricalPriors)

var dependentMeasures = ['comp_state','comp_allprob','wonkiness']
var quantifiers = ["Some","All","None"]

// var linkingFunction = function(marginalERP, measure, linkingBetaConcentration, wonkySoftmax){
// // var linkingFunction = cache(function(marginalERP, measure, sigma, wonkySoftmax){
//   Enumerate(function(){

//     // var queryStatement = sample(betaERP, [shape_alpha(posteriorProb, delta),
//     //                                        shape_beta(posteriorProb, delta)])

//     var discreteModelPredSliderVal = function(gamma, delta) {
//       var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
//       var discretizeBeta = function(gamma, delta){
//         var shape_alpha = gamma * delta
//         var shape_beta = (1-gamma) * delta
//         var betaPDF = function(x){
//           return Math.pow(x,shape_alpha-1)*
//               Math.pow((1-x),shape_beta-1)
//         }
//         return map(betaPDF, bins)
//       }
//       var quintileProbs = discretizeBeta(gamma,delta)
//       //return quintileProbs
//        var sliderVal = bins[discrete(quintileProbs)]
//        return sliderVal
//     }



//     var posteriorProb = measure == "comp_allprob" ? 
//                                 Math.exp(marginalERP.score([], 15)):
//                                wonkyutils.softmax(Math.exp(marginalERP.score([], true)),wonkySoftmax)
//                                 // Math.exp(marginalERP.score([], true))

//     var queryStatement = discreteModelPredSliderVal(posteriorProb,linkingBetaConcentration)

//     return queryStatement
//  //   return [posteriorProb, linkingBetaConcentration]
// }
// )}


var linkingFunction2 =  function(marginalERP, measure, sigma, offset, scale, phi, wonkySoftmax){
  Enumerate(function(){
   // var makeERPfromObject = function(obj){
   //  return Enumerate(
   //    function(){
   //      return wonkyutils.wpParseFloat(_.keys(obj)[discrete(_.values(obj))])
   //    })
   // }
    var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
    
    var logitNoiseLogistic = function(prob,sigma, offset, scale){
      var discreteGaussian=  map(function(d){
        return Math.exp(gaussianERP.score([wonkyutils.logit(prob),sigma], wonkyutils.logit(d)))
      }, bins)
      var noisyLogit_p =  wonkyutils.logit(bins[discrete(discreteGaussian)])
      var logisitic_noisy_p = wonkyutils.logistic(noisyLogit_p, offset, scale)
//      return discreteGaussian
     return wonkyutils.roundToBin(logisitic_noisy_p, bins)
      // return makeERPfromObject(linkedDist)
    }

    var posteriorProb = measure == "comp_allprob" ? 
                                Math.exp(marginalERP.score([], 15)):
                               wonkyutils.softmax(Math.exp(marginalERP.score([], true)),wonkySoftmax)
//                                Math.exp(marginalERP.score([], true))




    var posteriorProbAvoidEnds = (posteriorProb>0.99) ? 0.99 :
                                        (posteriorProb < 0.01) ? 0.01 :
                                              posteriorProb

    var queryStatement =   flip(phi) ?  uniformDraw(bins) : logitNoiseLogistic(posteriorProbAvoidEnds,sigma, offset, scale)

    return queryStatement
})
}




var queryDictionary = {'comp_state':"world",
                        'comp_allprob':"world",
                        'wonkiness':"wonky"}


var dataAnalysis = function(){

  // Priors (continuous)
  var speakerOptimality = uniform(0,10)
  var wonkinessPrior = uniform(0,1)
  var wonkySoftmax = uniform(0,1)
  var linkingSigma = uniform(0.1,2)
  var phi = uniform(0,1)
  var linkingOffset = {
     'comp_allprob':   uniform(-2,2),
      'wonkiness':   uniform(-2,2),
  }
  var linkingScale = {
     'comp_allprob':   uniform(0,2),
      'wonkiness':   uniform(0,2),
   }



  // Priors (discrete)

  // var speakerOptimality = 0.1+(randomInteger(20)/3)
  //  var wonkinessPrior = 0.01+(randomInteger(10)/10)
  //  var wonkySoftmax = 0.01+randomInteger(10)/3
  // var linkingSigma = (randomInteger(9)+1)/10
  // var phi = 0.01+(randomInteger(20)/20)
  //  var linkingOffset = {
  //    'comp_allprob':   (randomInteger(9)-5)/5,
  //     'wonkiness':   (randomInteger(9)-5)/5,
  //  }

  //  var linkingScale = {
  //    'comp_allprob':   (randomInteger(9)+1)/5,
  //     'wonkiness':   (randomInteger(9)+1)/5,
  //  }


  // var speakerOptimality = {
  //                       'comp_state':0.1+(randomInteger(20)/3),
  //                       'comp_allprob':0.1+(randomInteger(20)/3),
  //                       'wonkiness':0.1+(randomInteger(20)/3)
  // }

  //    var wonkinessPrior = 0.5
  // var wonkySoftmax = 1

 // var linkingBetaConcentration = (randomInteger(20)+1)/3
   // var linkingBetaConcentration = {
   //                      'comp_allprob':   (randomInteger(10)+1)/3,
   //                      'wonkiness':   (randomInteger(10)+1)/3
   //                    }

   // var linkingOffset = (randomInteger(9)-5)/5
   // var linkingScale = (randomInteger(9)+1)/5








   // var linkingSigma = {
   //                      'comp_allprob':   linkingSigmaOne,
   //                      'wonkiness':   linkingSigmaOne
   //                    }


  var posteriorPredictive = 
    map(function(item){
   //   console.log(item)
      return map(function(utterance){
     //   console.log(utterance)

        return map(function(measure){
   // //       console.log(measure)
        // console.log(item+measure+utterance)
   //      console.log(linkingSigma)

          var dataPoints = languageData[item][utterance][measure]
          var query = queryDictionary[measure]
          // console.log('prediction')
          // console.log(speakerOptimality+','+wonkinessPrior+','+_.values(linkingScale) +','+_.values(linkingOffset)+','+linkingSigma+','+phi)
          // console.log(dataPoints)
          // for model with just 1 speakerOptimality for all 3 tasks
         var predictionERP = listener(utterance, wonkinessPrior, speakerOptimality, item, query)
          // for speakerOptimalities that differ across tasks
//          var predictionERP = listener(utterance, wonkinessPrior, speakerOptimality[measure], item, query)


          // if comprehension task "give a number", use the marginal posterior directly
          // otherwise, need to connect to the slider value somehow
          // here, we assume the slider value is the mean of a beta (possibly discretized, for implementation)
          // and there is some concentration (delta) variable; this is a mere nuisance, data-analysis parameter
          var linkedERP = measure == "comp_state" ?
                           predictionERP :
                           // linkingFunction(predictionERP, measure, linkingBetaConcentration, wonkySoftmax)
                          // linkingFunction(predictionERP, measure, linkingBetaConcentration[measure], wonkySoftmax)
                            linkingFunction2(predictionERP, measure, linkingSigma, linkingOffset[measure], linkingScale[measure], phi, wonkySoftmax)

         // modelScore(linkedERP, dataPoints)==-Infinity ? console.log('error'):null
         factor(modelScore(linkedERP, dataPoints))
          // get posterior prob for each of the values in the support
           // return map(
           //  function(val){
           //    return [measure, item, utterance, val, Math.exp(linkedERP.score([], val))]
           //  },linkedERP.support())
           // get expectation of model predictions
           return [measure, item, utterance, expectation(linkedERP)]

          }, _.shuffle(dependentMeasures))

        }, //["Some"])//
          _.shuffle(quantifiers))

      }, //["sank marbles"])
     _.shuffle(listOf90Items))


 // return [speakerOptimality,wonkinessPrior,linkingBetaConcentration, ]
 // return [_.values(speakerOptimality),wonkinessPrior,_.values(linkingBetaConcentration),wonkySoftmax,
 //        _.flatten(_.flatten(_.flatten(posteriorPredictive, true),true),true)]

//return posteriorPredictive
 // return [_.values(speakerOptimality),wonkinessPrior,_.values(linkingSigma),
 //        _.flatten(_.flatten(_.flatten(posteriorPredictive, true),true),true)]
// return [_.values(speakerOptimality),wonkinessPrior,_.values(linkingSigma), wonkySoftmax]

//return speakerOptimality["wonkiness"]



 return [speakerOptimality,wonkinessPrior,phi,linkingSigma,  linkingOffset["comp_allprob"],linkingOffset["wonkiness"], 
 linkingScale["comp_allprob"],linkingScale["wonkiness"],wonkySoftmax,
 _.flatten(_.flatten(posteriorPredictive, true),true)]

        // return speakerOptimality

 // return _.flatten(_.flatten(_.flatten(posteriorPredictive, true),true),true)
}

// dataAnalysis()

 var mhsamples = 2000
var burn = 1000

// var opts = {
//   burn: 5,
//   skip: 0
// }


// var particles = 1000
var results = MH(dataAnalysis,mhsamples, burn, true)
//var results = IncrementalMH(dataAnalysis,mhsamples,opts)
// var results = ParticleFilter(dataAnalysis,particles)
//var results = Enumerate(dataAnalysis)

console.log('model complete. munging data...')


// wonkyutils.writePosteriorPredictiveWithParams(results)

// results.support()[0].slice(0,[results.support()[0].length-1])
// //dataAnalysis()

// // // console.log('here')
// // // var results = Enumerate(dataAnalysis)

// // // map(function(l){return Math.exp(results.score([],l))}, results.support())

//var parameterNames = ['speakerOptimality','wonkinessPrior','phi','linkingSigma','linkingOffset', 'linkingScale']
var parameterNames = ['speakerOptimality','wonkinessPrior','phi','linkingSigma','linkingOffset_allprob','linkingOffset_wonkiness', 'linkingScale_allprob','linkingScale_wonkiness',
'wonky_softmax']

var output = wonkyutils.writePosteriorPredictiveWithParams(results,parameterNames)
// // output.unshift([_.keys(queryDictionary).join('so,')+',wonkinessPrior,linkingBetaConcentration_allProb,linkingBetaConcentrationWonky,softmaxWonky,posteriorPredictive','posteriorProb']);
// output.unshift([_.keys(queryDictionary).join('so,')+',wonkinessPrior,linkingSigma_allProb,linkingSigmaWonky,posteriorPredictive','posteriorProb']);

// // //output.unshift(['speakerOptimality,wonkinessPrior,linkingBetaConcentration,posteriorPredictive','posteriorProb']);
// output.unshift(['speakerOptimality,wonkinessPrior,phi,linkingSigma,linkingOffset, linkingScale,posteriorPredictive','posteriorProb']);

// // output.unshift(['speakerOptimality,wonkinessPrior,linkingBetaConcentration,softmaxWonky','posteriorProb']);

//var outfile = 'results/wonkyFBTPosterior_1speakerOptimalities_2betas_mh'+ mhsamples+ 'b'+opts.burn+'.csv'
//var outfile = 'results/wonkyFBTPosterior_so_wp_phi_sigma_2offset_2scale_mh'+ mhsamples+ 'b'+opts.burn+'.csv'
var outfile = 'results/wonkyFBTPosterior_so_wp_phi_sigma_2offset_2scale_ws_CTS_mh'+ mhsamples+ 'b'+burn+'.csv'
// var outfile = 'results/wonkyFBTPosterior_so_wp_phi_sigma_2offset_2scale_CTS_pf'+particles+'.csv'


// var outfile = 'results/wonkyFBTPosterior_so_wp_phi_sigma_offset_scale_enum.csv'

//  'results/wonky_tfbtPosterior_predictiveAnddiscreteParams_softmaxWonky_mh'+ mhsamples+ 'b'+burn+'.csv'
// 'results/wonky_tfbtPosterior_predictiveAnddiscreteParams_3speakerOptimalities_2betas_softmaxWonky_mh'+ mhsamples+ 'b'+burn+'.csv'
// 'results/wonkyFBTPosterior_3speakerOptimalities_2sigmas_mh'+ mhsamples+ 'b'+opts.burn+'.csv'
//   'results/wonkyFBTPosterior_paramsOnly_1sOpt_lbc_wSmax_enumerate.csv'
//  'results/wonkyFBTPosterior_3sOpt_2lbc_wSmax_enumerate.csv'

wonkyutils.writeCSV(output, outfile)

console.log('wrote ' +outfile)











// console.log(languageData["sank marbles"]["Some"]["wonkiness"])

// 1.0105928825214505,1.3088386347517371,0.00007970025762915611,0.21598401363007724


          // console.log(linkingScale +','+linkingOffset+','+linkingSigma+','+phi)

          // console.log(_.values(linkingScale) +','+_.values(linkingOffset)+','+linkingSigma+','+phi)

// var utterance = "Some"
// var wonkinessPrior = 0.75
// var speakerOptimality = 0.66
// var item = "froze berries"
// var query = 'world'
// var measure = "comp_allprob"
// var predictionERP = listener(utterance, wonkinessPrior, speakerOptimality, item, query)

// //   // if comprehension task "give a number", use the marginal posterior directly
// //   // otherwise, need to connect to the slider value somehow
// //   // here, we assume the slider value is the mean of a beta (possibly discretized, for implementation)
// //   // and there is some concentration (delta) variable; this is a mere nuisance, data-analysis parameter


// var sigma = 0.1
// var offset = 1.0
// var scale = 1.3
// var phi = 0.21
// var linkedERP = linkingFunction2(predictionERP, measure, sigma, offset, scale,phi)
// linkedERP
// predictionERP


//     var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
    
//     var logitNoiseLogistic = function(prob,sigma, offset, scale){
//       var discreteGaussian=  map(function(d){
//         return Math.exp(gaussianERP.score([wonkyutils.logit(prob),sigma], wonkyutils.logit(d)))
//       }, bins)
//       var noisyLogit_p =  wonkyutils.logit(bins[discrete(discreteGaussian)])
//       var logisitic_noisy_p = wonkyutils.logistic(noisyLogit_p, offset, scale)
// //      return discreteGaussian
//      return wonkyutils.roundToBin(logisitic_noisy_p, bins)
//       // return makeERPfromObject(linkedDist)
//     }


// logitNoiseLogistic(0.14,sigma,offset,score)










  //   var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
  //   var prob = 0.13
  // var discreteGaussian=  map(function(d){
  //   return Math.exp(gaussianERP.score([wonkyutils.logit(prob),sigma], wonkyutils.logit(d)))
  // }, bins)

  // discreteGaussian

// Math.exp(gaussianERP.score([wonkyutils.logit(prob),sigma], wonkyutils.logit(0.99)))
// console.log(Math.exp(predictionERP.score([],15)))
// linkedERP
// map(
//   function(x){
//     return wonkyutils.logistic(wonkyutils.logit(x), 0, 1)
//   },
//   [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])
// predictionERP