// time webppl wonky-tfbt.wppl --require-js ./wonkyutils --require-wppl wonkyRSA.wppl 
//--compile --out tfbt_6paramsHash_070615.js

// time node --max-old-space-size=3072 tfbt_6paramsHash_070615.js

// helper functions

var foreach = function(lst, fn) {
    var foreach_ = function(i) {
        if (i < lst.length) {
            fn(lst[i]);
            foreach_(i + 1);
        }
    };
    foreach_(0);
};

var marginalize = function(myERP, index){
  Enumerate(function(){
    var x = sample(myERP)
    return x[index]
  })
}

var subset = function(df, field, value){
  return filter(function(d){
    return (d[field]==value)
  },df)
}



var modelScore = function(modelERP, data){
    return reduce(function(dataPoint, memo) {
                return memo + modelERP.score([], dataPoint);
            }, 0, data)
  }


// data
var empiricalPriors = wonkyutils.readPriors()
var languageData = wonkyutils.readData()

var listOf90Items = _.keys(empiricalPriors)

var dependentMeasures = ['comp_state','comp_allprob','wonkiness']
var quantifiers = ["Some","All","None"]

// var linkingFunction = function(marginalERP, measure, linkingBetaConcentration, wonkySoftmax){
// // var linkingFunction = cache(function(marginalERP, measure, sigma, wonkySoftmax){
//   Enumerate(function(){

//     // var queryStatement = sample(betaERP, [shape_alpha(posteriorProb, delta),
//     //                                        shape_beta(posteriorProb, delta)])

//     var discreteModelPredSliderVal = function(gamma, delta) {
//       var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
//       var discretizeBeta = function(gamma, delta){
//         var shape_alpha = gamma * delta
//         var shape_beta = (1-gamma) * delta
//         var betaPDF = function(x){
//           return Math.pow(x,shape_alpha-1)*
//               Math.pow((1-x),shape_beta-1)
//         }
//         return map(betaPDF, bins)
//       }
//       var quintileProbs = discretizeBeta(gamma,delta)
//       //return quintileProbs
//        var sliderVal = bins[discrete(quintileProbs)]
//        return sliderVal
//     }



//     var posteriorProb = measure == "comp_allprob" ? 
//                                 Math.exp(marginalERP.score([], 15)):
//                                wonkyutils.softmax(Math.exp(marginalERP.score([], true)),wonkySoftmax)
//                                 // Math.exp(marginalERP.score([], true))

//     var queryStatement = discreteModelPredSliderVal(posteriorProb,linkingBetaConcentration)

//     return queryStatement
//  //   return [posteriorProb, linkingBetaConcentration]
// }
// )}


var linkingFunction2 =  function(marginalERP, measure, sigma, offset, scale, phi){
  Enumerate(function(){
   // var makeERPfromObject = function(obj){
   //  return Enumerate(
   //    function(){
   //      return wonkyutils.wpParseFloat(_.keys(obj)[discrete(_.values(obj))])
   //    })
   // }


    var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
    var states = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
    
    var logitNoiseLogistic = function(prob,sigma, offset, scale){
      var discreteGaussian=  map(function(d){
        return Math.exp(gaussianERP.score([wonkyutils.logit(prob),sigma], wonkyutils.logit(d)))
      }, bins)
      var noisyLogit_p =  wonkyutils.logit(bins[discrete(discreteGaussian)])
      var logisitic_noisy_p = wonkyutils.logistic(noisyLogit_p, offset, scale)
//      return discreteGaussian
     return wonkyutils.roundToBin(logisitic_noisy_p, bins)
      // return makeERPfromObject(linkedDist)
    }


    var posteriorProb = measure == "comp_allprob" ? 
                                Math.exp(marginalERP.score([], 15)):
                               // wonkyutils.softmax(Math.exp(marginalERP.score([], true)),wonkySoftmax)
                               Math.exp(marginalERP.score([], true))




    var posteriorProbAvoidEnds = (posteriorProb>0.99) ? 0.99 :
                                        (posteriorProb < 0.01) ? 0.01 :
                                              posteriorProb

    var queryStatement =  flip(phi) ?  (measure == "comp_allprob") ? 
                                                  uniformDraw(bins) : 
                                        (measure == "comp_state") ? 
                                                    uniformDraw(states) :
                                                    //flip(0.5) ? 1:0 :
                                                    uniformDraw(bins) :
                                       (measure == "comp_allprob") ? 
                                                    // wonkyutils.roundToBin(posteriorProbAvoidEnds, bins) :
                                                  // wonkyutils.roundToBin(wonkyutils.logitLogistic(posteriorProbAvoidEnds, offset, scale), bins) :
                                                   logitNoiseLogistic(posteriorProbAvoidEnds,sigma, offset, scale) : 
                                        (measure == "comp_state") ? 
                                                      sample(marginalERP) : 
                                                      // logitNoiseLogistic(posteriorProbAvoidEnds,wonkySigma, wonkyOffset, wonkyScale)
                                                        flip(wonkyutils.logitLogistic(posteriorProb, 
                                                                                        offset, 
                                                                                        scale))?1:0

    return queryStatement
})
}




var queryDictionary = {'comp_state':"world",
                        'comp_allprob':"world",
                        'wonkiness':"wonky"}


var dataAnalysis = function(){

  // Priors (continuous)
  var speakerOptimality = uniform(0,8)
  var wonkinessPrior = uniform(0,1)
  var phi = uniform(0,1)

  foreach( _.shuffle(dependentMeasures),
    function(measure){

      var sigma = measure=='comp_allprob' ? exponential(1)+0.1 : 0
      // var linkingSigma = 0
      var scale =  measure=='comp_allprob' ? exponential(1) : 1
      var offset = measure=='comp_allprob' ? sample(gaussianDriftERP,[0,1]) : 0

      foreach(_.shuffle(listOf90Items),
        function(item){

        foreach(_.shuffle(quantifiers),
          function(utterance){

          var dataPoints = languageData[item][utterance][measure]
          var predictionERP = listener(utterance, wonkinessPrior, speakerOptimality, item, queryDictionary[measure])
          // if comprehension task "give a number", use the marginal posterior directly
          // otherwise, need to connect to the slider value somehow
          // here, we assume the slider value is the mean of a beta (possibly discretized, for implementation)
          // and there is some concentration (delta) variable; this is a mere nuisance, data-analysis parameter
          var linkedERP = linkingFunction2(predictionERP, 
                                              measure, 
                                              sigma, 
                                              offset, 
                                              scale, 
                                              phi)


            // modelScore(linkedERP, dataPoints)==-Infinity ? console.log('error'):null
            var scr = modelScore(linkedERP, dataPoints)
            factor(scr)

            query.add(["measure","item","utterance"] ,expectation(linkedERP))
            // get posterior prob for each of the values in the support
           // return map(
           //  function(val){
           //    return [measure, item, utterance, val, Math.exp(linkedERP.score([], val))]
           //  },linkedERP.support())
           // get expectation of model predictions
           // return [measure, item, utterance, expectation(linkedERP)]

          })

        })

      })


 // return [speakerOptimality,wonkinessPrior,linkingBetaConcentration, ]
 // return [_.values(speakerOptimality),wonkinessPrior,_.values(linkingBetaConcentration),wonkySoftmax,
 //        _.flatten(_.flatten(_.flatten(posteriorPredictive, true),true),true)]

//return posteriorPredictive
 // return [_.values(speakerOptimality),wonkinessPrior,_.values(linkingSigma),
 //        _.flatten(_.flatten(_.flatten(posteriorPredictive, true),true),true)]
// return [_.values(speakerOptimality),wonkinessPrior,_.values(linkingSigma), wonkySoftmax]

//return speakerOptimality["wonkiness"]



 // return [speakerOptimality,wonkinessPrior,phi,linkingSigma,  linkingOffset["comp_allprob"],linkingOffset["wonkiness"], 
 // linkingScale["comp_allprob"],linkingScale["wonkiness"],wonkySoftmax,
 // _.flatten(_.flatten(posteriorPredictive, true),true)]
// ['speakerOptimality','wonkinessPrior','phi']
 return query

    // [ ['speakerOptimality','na','na',speakerOptimality],
    //       ['wonkinessPrior','na','na',wonkinessPrior],
    //       ['phi','na','na',phi],
    //       ["allprob_scale","na","na",linkingScale],
    //       ["allprob_offset","na","na",linkingOffset],
    //       ["allprob_sigma","na","na",linkingSigma],
    //       ["wonkiness_scale","na","na",wonkyScale],
    //       ["wonkiness_offset","na","na",wonkyOffset],
    //       ["wonkiness_sigma","na","na",wonkySigma],
    //       _.flatten(_.flatten(posteriorPredictive, true),true) ]

 // return [speakerOptimality,wonkinessPrior,phi["comp_state"],phi["comp_allprob"],phi["wonkiness"],linkingSigma,linkingOffset,linkingScale,wonkySoftmax,
 // _.flatten(_.flatten(posteriorPredictive, true),true)]

        // return speakerOptimality

 // return _.flatten(_.flatten(_.flatten(posteriorPredictive, true),true),true)
}

var mhiter = 50
//var burn = 25000
// var particles = 1000
//var results = MH(dataAnalysis,mhsamples, burn, true)

var results = IncrementalMH(dataAnalysis, mhiter)
//var results = HashMH(dataAnalysis, mhiter)

results

//var results = IncrementalMH(dataAnalysis,mhsamples,opts)
// var results = ParticleFilter(dataAnalysis,particles)
//var results = Enumerate(dataAnalysis)
// console.log('model complete. munging data...')
// //var outfile = 'results/wonkyFBTPosterior_wonkyTF_so_wp_phi_allProb-scale-offset-sigma_CTS_hashMH'+ mhsamples+ 'b'+burn+'.csv'
// //var outfile = 'results/wonkyFBTPosterior_wonkyTF_so_wp_phi_allProb-scale-offset-sigma_CTS_incrMH'+ mhiter+'.csv'
// //var outfile = 'results/wonkyFBTPosterior_wonkyTF_so_wp_phi_allProb-scale-offset-sigma_CTS_hashMH'+ mhiter+'compiled.csv'
// //var outfile = 'results/wonkyFBTPosterior_wonkyTF_so_wp_phi_allProb-scale-offset-sigma_wonkiness-scale-offset_CTS_hashMH'+ mhiter+'compiled.csv'
// //var outfile = 'results/wonkyFBTPosterior_wonkyTF_so_wp_phi_allProb-scale-offset_wonkiness-scale-offset_CTS_hashMH'+ mhiter+'compiled.csv'
// //var outfile = 'results/wonkyFBTPosterior_wonkyTF_so_wp_phi_allProb-scale-offset-sigma_wonkiness-scale_CTS_hashMH'+ mhiter+'compiled.csv'
// var outfile = 'results/wonkyFBTPosterior_wonkySlider_so_wp_phi_allProb-scale-offset-sigma_wonkiness-scale-offset-sigma_CTS_incrMH'+ mhiter+'c.csv'

// wonkyutils.erpWriter(results, outfile)
// console.log('wrote ' +outfile)

// //OUTPUT

//var parameterNames = ['speakerOptimality','wonkinessPrior','phi','linkingSigma','linkingOffset', 'linkingScale']
// var parameterNames = ['speakerOptimality','wonkinessPrior','phi','linkingSigma','linkingOffset_allprob','linkingOffset_wonkiness', 'linkingScale_allprob','linkingScale_wonkiness',
// 'wonky_softmax']
// var parameterNames = 
// var parameterNames = ['speakerOptimality','wonkinessPrior','phi_state','phi_allprob', 'phi_wonkiness','linkingSigma','linkingOffset_allprob', 'linkingScale_allprob','wonky_softmax']


// var output = wonkyutils.writePosteriorPredictiveWithParams(results,parameterNames)

// // output.unshift([_.keys(queryDictionary).join('so,')+',wonkinessPrior,linkingBetaConcentration_allProb,linkingBetaConcentrationWonky,softmaxWonky,posteriorPredictive','posteriorProb']);
// output.unshift([_.keys(queryDictionary).join('so,')+',wonkinessPrior,linkingSigma_allProb,linkingSigmaWonky,posteriorPredictive','posteriorProb']);
// // //output.unshift(['speakerOptimality,wonkinessPrior,linkingBetaConcentration,posteriorPredictive','posteriorProb']);
// output.unshift(['speakerOptimality,wonkinessPrior,phi,linkingSigma,linkingOffset, linkingScale,posteriorPredictive','posteriorProb']);
// // output.unshift(['speakerOptimality,wonkinessPrior,linkingBetaConcentration,softmaxWonky','posteriorProb']);
//var outfile = 'results/wonkyFBTPosterior_1speakerOptimalities_2betas_mh'+ mhsamples+ 'b'+opts.burn+'.csv'
//var outfile = 'results/wonkyFBTPosterior_so_wp_phi_sigma_2offset_2scale_mh'+ mhsamples+ 'b'+opts.burn+'.csv'
//var outfile = 'results/wonkyFBTPosterior_so_wp_phi_sigma_2offset_2scale_ws_CTS_mh'+ mhsamples+ 'b'+burn+'.csv'
//var outfile = 'results/wonkyFBTPosterior_wonkyTF_so_wp_phi_sigma_offset_scale_ws_CTS_mh'+ mhsamples+ 'b'+burn+'.csv'
// var outfile = 'results/wonkyFBTPosterior_wonkyTF_so_wp_3xPhi_sigma_offset_scale_ws_CTS_mh'+ mhsamples+ 'b'+burn+'.csv'

// var outfile = 'results/wonkyFBTPosterior_so_wp_phi_sigma_2offset_2scale_CTS_pf'+particles+'.csv'
// var outfile = 'results/wonkyFBTPosterior_so_wp_phi_sigma_offset_scale_enum.csv'
//  'results/wonky_tfbtPosterior_predictiveAnddiscreteParams_softmaxWonky_mh'+ mhsamples+ 'b'+burn+'.csv'
// 'results/wonky_tfbtPosterior_predictiveAnddiscreteParams_3speakerOptimalities_2betas_softmaxWonky_mh'+ mhsamples+ 'b'+burn+'.csv'
// 'results/wonkyFBTPosterior_3speakerOptimalities_2sigmas_mh'+ mhsamples+ 'b'+opts.burn+'.csv'
//   'results/wonkyFBTPosterior_paramsOnly_1sOpt_lbc_wSmax_enumerate.csv'
//  'results/wonkyFBTPosterior_3sOpt_2lbc_wSmax_enumerate.csv'

// wonkyutils.writeCSV(output, outfile)






// wonkyutils.writePosteriorPredictiveWithParams(results)
// results.support()[0].slice(0,[results.support()[0].length-1])
// //dataAnalysis()
// // // console.log('here')
// // // var results = Enumerate(dataAnalysis)
// // // map(function(l){return Math.exp(results.score([],l))}, results.support())






// 1.0105928825214505,1.3088386347517371,0.00007970025762915611,0.21598401363007724


          // console.log(linkingScale +','+linkingOffset+','+linkingSigma+','+phi)

          // console.log(_.values(linkingScale) +','+_.values(linkingOffset)+','+linkingSigma+','+phi)

// var utterance = "Some"
// var wonkinessPrior = 0.29
// var speakerOptimality = 4.5
// // var item = "sank marbles"
// var item = "stuck to the wall baseballs"
// var item = "sank balloons"
// var query = 'wonky'
// var measure = "wonkiness"
// var predictionERP = listener(utterance, wonkinessPrior, speakerOptimality, item, query)

// //   // if comprehension task "give a number", use the marginal posterior directly
// //   // otherwise, need to connect to the slider value somehow
// //   // here, we assume the slider value is the mean of a beta (possibly discretized, for implementation)
// //   // and there is some concentration (delta) variable; this is a mere nuisance, data-analysis parameter


// var sigma = 1.8
// var offset = 0.6
// var scale = 1.1
// var phi = 0.11
// var wonkyoffset = -.5
// var wonkyScale = 1.5
// var linkedERP = linkingFunction2(predictionERP, measure, sigma, offset, scale, phi, wonkyoffset, wonkyScale)


// console.log(languageData[item][utterance][measure])
// console.log(empiricalPriors[item])

// console.log(Math.exp(linkedERP.score([],1)))



// var item2 = "melted ice cubes"
// var predictionERP2 = listener(utterance, wonkinessPrior, speakerOptimality, item2, query)
// var linkedERP2 = linkingFunction2(predictionERP2, measure, sigma, offset, scale, phi, wonkyoffset, wonkyScale)


// console.log(languageData[item2][utterance][measure])
// console.log(empiricalPriors[item2])
// console.log(Math.exp(linkedERP2.score([],1)))


// predictionERP

//     var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
    
//     var logitNoiseLogistic = function(prob,sigma, offset, scale){
//       var discreteGaussian=  map(function(d){
//         return Math.exp(gaussianERP.score([wonkyutils.logit(prob),sigma], wonkyutils.logit(d)))
//       }, bins)
//       var noisyLogit_p =  wonkyutils.logit(bins[discrete(discreteGaussian)])
//       var logisitic_noisy_p = wonkyutils.logistic(noisyLogit_p, offset, scale)
// //      return discreteGaussian
//      return wonkyutils.roundToBin(logisitic_noisy_p, bins)
//       // return makeERPfromObject(linkedDist)
//     }


// logitNoiseLogistic(0.14,sigma,offset,score)









  //   var bins = [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]
  //   var prob = 0.13
  // var discreteGaussian=  map(function(d){
  //   return Math.exp(gaussianERP.score([wonkyutils.logit(prob),sigma], wonkyutils.logit(d)))
  // }, bins)

  // discreteGaussian

// Math.exp(gaussianERP.score([wonkyutils.logit(prob),sigma], wonkyutils.logit(0.99)))
// console.log(Math.exp(predictionERP.score([],15)))
// linkedERP
// map(
//   function(x){
//     return wonkyutils.logistic(wonkyutils.logit(x), 0, 1)
//   },
//   [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])
// predictionERP