Wonky marbles follow-ups -- the journey to the journal
========================

What do we need to do to make a journal article out of the CogSci paper? 

Writing
-------

I think the framing and structure of the CogSci paper are solid. We'll have to flesh out the intro some more, making more connections to what people have said in psycholinguistics about weird items (eg Alex and Florian's work for actual models of belief update over the course of an experiment in response to wonky distributions; look at Gail Mauner's work for less formal stuff/quotes).

We need to flesh out the model comparison section: compare regular RSA to wonky RSA to a model that uses just a uniform prior. Show that wonky RSA does best.

Discussion: spell out common ground/accommodation issue much more. 


Modeling
--------

Figure out what the best backoff-prior is -- currently uniform does best. 

- binomial didn't do great, maybe because the actual priors aren't very close to binomials? fit binomials to empirical priors and try again?

- what are other reasonable priors?


Empirical
---------

The currently reported experiments estimate both priors and posteriors on 15 objects. Should we run larger versions of the experiments where we get data on different numbers of total objects? Eg 4, 7, 10, 15. This would make the results (if they hold up) more general (and we already have some noisy data suggesting that things don't differ very much across different total numbers of objects).

The wonkiness estimate experiment that we reported asked for "How likely is it that these are normal marbles?" and people adjusted a slider from "definitely not normal" to "definitely normal". We also did the same thing with 
- 2AFC "Do you think these are normal marbles?" and 
- 2AFC "Do you think Laura is describing a normal event?" combined with a free production "If you said no, what do you think is not normal about it?" task

In both of these experiments, we got the same results as with the sliders, except wonkiness judgments were lower in the mid prior range. Do we need an experiment that asks about the wonkiness of the speaker or asks about wonkiness in some other way? My intuition is that we don't.

Do we need to weed out some of the weirder items?

Data analysis
-------------

Can we do better than reporting mean squared error for each of the models? Posterior predictive?


Potential follow-ups
----------

### Common ground/accommodation

The common ground/accommodation issue raises some really interesting questions: what exactly are we revising when we think the world is wonky? Options:

- our beliefs about the entire event or just the objects involved; can be teased apart by setting up contexts in which the same objects (or objects of the same type, from the same source) participate in a second event, though I don't think this is the most interesting question

- our beliefs about the speaker -- maybe the speaker is unreliable/uncooperative (eg maybe he systematically over- or understates his case); or maybe he uses language differently.

Whether listeners are updating their beliefs about speakers or about the world I think is an interesting question to follow up on for both modeling and experiments.

### Prior manipulation

Many of the items we used, people have strong prior beliefs/world knowledge about. Would our case be stronger if we created items that people don't have strong prior beliefs about, e.g. marbles in a box being red, and manipulate the prior by exposing people to different boxes before presenting them with an utterance?

### QUD

Now that we think we understand better what's going on with the prior, can we revisit our old QUD cases and figure out whether wRSA actually predicts the null effects we got, and what situations it predicts non-null-effects for?

### Beyond quantifiers

Is there a way we can extend this general paradigm to other scalar items?