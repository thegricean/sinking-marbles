---
title: "wrsa-rsa-model-comparison"
author: "M. H. Tessler"
date: "October 26, 2016"
output: pdf_document
---

```{r global_options, include=FALSE}
library(knitr)
knitr::opts_chunk$set(#fig.width=7, fig.height=5, 
                      fig.crop = F,
                      echo=FALSE, warning=FALSE, cache=F, message=FALSE, sanitize = T)

library(rwebppl)
library(dplyr)
```


Prior model results

```{r priorModelResults, fig.width = 10, fig.height = 15}
load("~/Documents/research/sinking-marbles/models/wrsa/results/items_mcmc20k.Rdata")

ggplot(m.post, aes(x = bin, y = MAP,
           ymin = cred_low, ymax = cred_high))+
  geom_bar(stat = 'identity', position = position_dodge())+
  geom_errorbar() + 
  facet_wrap(~item)
```


# Language understanding

## Regular RSA

Using the MAP estimates for the priors, we run RSA (inside a BDA model to infer `alpha`, the speaker optimality). As I mentioned above, we are only observing the comp_state data (because we can compute the score from RSA directly [no linking function]). Nonetheless, we will examine the posterior predictive over the allprob data (all state probability) and the wonkiness judgment (for wRSA).

```{r}
wRSA <- '
var allPriors = myData["priors"]
var allData = myData["data"]

var domainPrior = function(probs){
  return discrete(probs)
}

// possible utterances
var utterancePrior = function() {
  return uniformDraw(["all", "some", "none"]);
};

// meaning funtion to interpret the utterances
var literalMeanings = {
  all: function(state) { return state === 15; },
  some: function(state) { return state > 0; },
  none: function(state) { return state === 0; }
};

// literal listener
var literalListener = cache(function(utt, itemPrior) {
  return Infer({method:"enumerate"},
  function(){
    var state = domainPrior(itemPrior)
    var meaning = literalMeanings[utt]
    condition(meaning(state))
    return state
  })
}, 10000);

// pragmatic speaker
var speaker = cache(function(state, alpha, itemPrior) {
  return Infer({method:"enumerate"},
  function(){
    var utt = utterancePrior()
    factor(alpha * literalListener(utt, itemPrior).score(state))
    return utt
  })
}, 10000);

// pragmatic listener
var pragmaticListener = function(utt, alpha, itemPrior) {
  return Infer({method:"enumerate"},
  function(){
    var state = domainPrior(itemPrior)
    observe(speaker(state, alpha, itemPrior), utt)
    return state
  })
}

var items = _.uniq(_.pluck(allPriors, "item"));

var dataAnalysis = function(){
  // var alpha = uniformDrift({a: 0, b:20, width: 0.2});
  var alpha = uniformDraw(_.range(0.5, 20.5, 1));
  display(alpha)
  
  var predictives = map(function(i){
    // webppl objects sorted alphanumerically (so can just pluck)
    // otherwise, would need to ensure that bins correspond to 0-15
    var itemPrior = _.pluck(_.where(allPriors, {item: i}), "MAP");
    var itemData =  _.pluck(_.where(allData, {item: i}), "response");
    var l1Predictions = pragmaticListener("some", alpha, itemPrior);

    observeData({
      data: itemData,
      link: l1Predictions
    })

    
    return [ i, {expectation: expectation(l1Predictions), allprob: probability(15, l1Predictions) } ]
  }, items)

  return _.object(predictives.concat([["alpha", alpha]]))
}
'
```

### Run RSA-BDA model

Using only comp_state data, for "Some", censoring out the responses that are 0.

```{r loadData}
d0<-read.csv("~/Documents/research/sinking-marbles/bayesian_model_comparison/data/empirical.csv")
str(d0)
d0.state <- d0 %>% filter(measure == "comp_state")
str(d0.state)
```

There are `r length(d0.state[,1])` observations for "Some" in the comp_state data. 
That is on average: `r round(length(d0.state[,1]))/90` observations per item.

```{r runRSA, echo=T, eval=F}
load("~/Documents/research/sinking-marbles/models/wrsa/results/items_mcmc20k.Rdata")

dataToPass = list(priors = m.post,
                  data = d0.state %>% filter(response != 0 ))

bda.results <- webppl(RSA, 
       data_var = "myData",
       data = dataToPass,
       packages = c("mht"),
       inference_opts = list(method = "enumerate"),
       model_var = "dataAnalysis")

 # save(bda.results,
 #      file = "results/rsa_mcmc1kb0.5k_x2.Rdata")
```


```{r processRSAresults}
#load("~/Documents/research/sinking-marbles/results/rsa_mcmc1kb0.5k_x2.Rdata")

bda.results %>%
  select(alpha, prob) %>%
  ggplot(., aes(x = alpha, y = prob))+
  geom_bar(stat = 'identity', position = position_dodge())


post.pred <- bda.results %>%
  select(-alpha) %>%
  gather(key, val, -prob) %>%
  separate(key, into = c("item", "param"), sep ='[.]') %>%
  group_by(param, item) %>%
  summarize(expval = sum(val*prob))


d.summ <- d0.state %>%
  group_by(item, utterance, measure) %>%
  multi_boot_standard(column = "response")
```


### Model-data scatter

```{r}
md <- left_join(d.summ, post.pred %>% filter(param == "expectation"))

ggplot(md, aes(x = expval, #xmin = credLo, xmax = credHi,
               y = mean, ymin = ci_lower, ymax = ci_upper))+
  geom_point()+
  xlim(0, 15) + 
  ylim(0, 15) + 
  coord_fixed() + 
  geom_errorbar(alpha = 0.1) + 
 # geom_errorbarh(alpha = 0.1)+
  geom_abline(intercept = 0, slope = 1, linetype = 3)
```

$$r^2(`r nrow(md)`) = `r with(md, cor(mean, MAP))^2`$$

## Paper figures (Interpretation vs. Prior expectation)

Give a number

```{r}
# prior data
head(m.post)

md.wPrior <- left_join(
  bind_rows(post.pred %>% filter(param == "expectation") %>%
            rename(response = expval) %>% #, ci_lower = credLo, ci_upper = credHi) %>% 
              ungroup() %>%
              select(-param) %>% mutate(src = "model"),
          d.summ %>% rename(response = mean) %>% ungroup() %>%
            select(-utterance, -measure) %>% mutate(src = "data")),
  m.post %>%
    select(MAP, item, bin) %>%
    rename(priorMAP = MAP) %>%
    group_by(item) %>%
    summarize(priorExpectation = sum((bin - 1)*priorMAP))
)

ggplot(md.wPrior, aes( x = priorExpectation, y = response) +#, ymin = ci_lower, ymax = ci_upper ))+
  geom_point()+
  facet_wrap(~src)
 # geom_errorbar(alpha = 0.1)
```

All state probability

```{r}
d0.allprob <- d0 %>% filter(measure == "comp_allprob")
d.allprob.summ <- d0.allprob %>%
  group_by(item, utterance, measure) %>%
  multi_boot_standard(column = "response")

# prior data
head(m.post)

mdAllProb.wPrior <- left_join(
  bind_rows(
    post.pred %>% filter(param == "allprob") %>%
            rename(response = expval) %>% #, ci_lower = credLo, ci_upper = credHi) %>% 
              ungroup() %>%
              select(-param) %>% mutate(src = "model"),
           d.allprob.summ %>% rename(response = mean) %>% ungroup() %>%
             select(-utterance, -measure) %>% mutate(src = "data")
           ),
  m.post %>%
    filter(bin == 16) %>%
    select(MAP, item) %>%
    rename(priorMAP = MAP)
)

ggplot(mdAllProb.wPrior, aes( x = priorMAP, y = response))+#, ymin = ci_lower, ymax = ci_upper ))+
  geom_point()+
  facet_wrap(~src)+
  #geom_errorbar(alpha = 0.1)+
  geom_smooth()
```


## Wonky RSA 1: `wonky = flip()`, `backoff = uniform(...)`

We now try a wonky RSA model with a fixed wonkyPrior probability of 0.5, and a uniform back off prior.

```{r wRSA, echo=T}
wRSA <- '
var allPriors = myData["priors"]
var allData = myData["data"]

var backoffPrior = mht.fillArray(1, 16);

var domainPrior = function(probs){
  return discrete(probs)
}

// possible utterances
var utterancePrior = function() {
  return uniformDraw(["all", "some", "none"]);
};

// meaning funtion to interpret the utterances
var literalMeanings = {
  all: function(state) { return state === 15; },
  some: function(state) { return state > 0; },
  none: function(state) { return state === 0; }
};

// literal listener
var literalListener = cache(function(utt, prior) {
  return Infer({method:"enumerate"},
  function(){
    var state = domainPrior(prior)
    var meaning = literalMeanings[utt]
    condition(meaning(state))
    return state
  })
}, 10000);

// pragmatic speaker
var speaker = cache(function(state, alpha, prior) {
  return Infer({method:"enumerate"},
  function(){
    var utt = utterancePrior()
    factor(alpha * literalListener(utt, prior).score(state))
    return utt
  })
}, 10000);

// pragmatic listener
var pragmaticListener = function(utt, alpha, wonkyPrior, itemPrior) {
  return Infer({method:"enumerate"},
  function(){
    var wonky = flip(wonkyPrior);
    var prior = wonky ? backoffPrior : itemPrior
    var state = domainPrior(prior)
    observe(speaker(state, alpha, prior), utt)
    return {state: state, wonky: wonky}
  })
}

var items = _.uniq(_.pluck(allPriors, "item"));

var dataAnalysis = function(){
  var alpha = uniformDraw(_.range(0.5, 20.5, 1));
  display(alpha)
  //var wonkyPrior = uniformDrift({a:0, b:1, width: 0.1});
  var wonkyPrior = 0.5;
  
  var predictives = map(function(i){
    // webppl objects sorted alphanumerically (so can just pluck)
    // otherwise, would need to ensure that bins correspond to 0-15
    var itemPrior = _.pluck(_.where(allPriors, {item: i}), "MAP");
    var itemData =  _.pluck(_.where(allData, {item: i}), "response");

    var l1Predictions = pragmaticListener("some", alpha, wonkyPrior, itemPrior);

    var l1State = marginalize(l1Predictions, "state");
    var l1Wonky = marginalize(l1Predictions, "wonky");

    observeData({
      data: itemData,
      link: l1State
    })

    return [ i, {expectation: expectation(l1State), allprob: probability(15, l1State), wonky: expectation(l1Wonky) } ]
  }, items)

  return _.object(predictives.concat([["alpha", alpha]]))

}
'
```
Using only comp_state data, for "Some", censoring out the responses that are 0.

```{r eval=F}
# load("models/wrsa/results/items_mcmc20k.Rdata")
# d0<-read.csv("bayesian_model_comparison/data/empirical.csv")
# d0.state <- d0 %>% filter(measure == "comp_state")
# dataToPass = list(priors = m.post,
#                   data = d0.state %>% filter(response != 0 ))

bda.results <- webppl(wRSA, 
       data_var = "myData",
       data = dataToPass,
       packages = c("mht"),
       inference_opts = list(method = "enumerate"),
       model_var = "dataAnalysis")

#save(bda.results,
#      file = "models/wrsa/results/wrsa1_mcmc1kb0.5k_x2.Rdata")
```

### Posterior on alpha

```{r}
#load("~/Documents/research/sinking-marbles/models/wrsa/results/wrsa1_mcmc1kb0.5k_x2.Rdata")

bda.results %>%
  select(alpha, prob) %>%
  ggplot(., aes(x = alpha, y = prob))+
  geom_bar(stat = 'identity', position = position_dodge())


post.pred <- bda.results %>%
  select(-alpha) %>%
  gather(key, val, -prob) %>%
  separate(key, into = c("item", "param"), sep ='[.]') %>%
  group_by(param, item) %>%
  summarize(expval = sum(val*prob))

```


### Model-data scatter

```{r}
md <- left_join(d.summ, post.pred %>% filter(param == 'expectation'))

ggplot(md, aes(x = expval,
               y = mean, ymin = ci_lower, ymax = ci_upper))+
  geom_point()+
  xlim(0, 15) + 
  ylim(0, 15) + 
  coord_fixed() + 
  geom_errorbar(alpha =0.1) + 
  #geom_errorbarh(alpha = 0.1)+
  geom_abline(intercept = 0, slope = 1, linetype = 3)
```

$$r^2(`r nrow(md)`) = `r with(md, cor(mean, MAP))^2`$$


```{r eval=F}
post.pred %>% filter(param == "wonky") %>%
  ggplot(., aes(x = expval))+
  geom_histogram()+
  ggtitle("Distribution on wonky ratings")
```

## Paper figures (Interpretation vs. Prior expectation)

Give a number

```{r}
# prior data
head(m.post)

md.wPrior <- left_join(
  bind_rows(post.pred %>% filter(param == "expectation") %>%
            rename(response = expval ) %>% #, ci_lower = credLo, ci_upper = credHi) %>% 
              ungroup() %>%
              select(-param) %>% mutate(src = "model"),
          d.summ %>% rename(response = mean) %>% ungroup() %>%
            select(-utterance, -measure) %>% mutate(src = "data")),
  m.post %>%
    select(MAP, item, bin) %>%
    rename(priorMAP = MAP) %>%
    group_by(item) %>%
    summarize(priorExpectation = sum((bin - 1)*priorMAP))
)

ggplot(md.wPrior, aes( x = priorExpectation, y = response, ymin = ci_lower, ymax = ci_upper ))+
  geom_point()+
  facet_wrap(~src)+
  geom_errorbar(alpha = 0.1)
```

All prob

```{r}
# prior data
head(m.post)

mdAllProb.wPrior <- left_join(
  bind_rows(
    post.pred %>% filter(param == "allprob") %>%
            rename(response = expval ) %>%#, ci_lower = credLo, ci_upper = credHi) %>% 
              ungroup() %>%
              select(-param) %>% mutate(src = "model"),
           d.allprob.summ %>% rename(response = mean) %>% ungroup() %>%
             select(-utterance, -measure) %>% mutate(src = "data")
           ),
  m.post %>%
    filter(bin == 16) %>%
    select(MAP, item) %>%
    rename(priorMAP = MAP)
)

ggplot(mdAllProb.wPrior, aes( x = priorMAP, y = response) ) + #, ymin = ci_lower, ymax = ci_upper ))+
  geom_point()+
  facet_wrap(~src)+
  #geom_errorbar(alpha = 0.1)+
  geom_smooth()
```

Wonkiness

```{r}

d0.wonky <- d0 %>% filter(measure == "wonkiness")
d.wonky.summ <- d0.wonky %>%
  group_by(item, utterance, measure) %>%
  multi_boot_standard(column = "response")

mdWonky.wPrior <- left_join(
  bind_rows(
    post.pred %>% filter(param == "wonky") %>%
            rename(response = expval ) %>% #, ci_lower = credLo, ci_upper = credHi) %>% 
              ungroup() %>%
              select(-param) %>% mutate(src = "model"),
           d.wonky.summ %>% rename(response = mean) %>% ungroup() %>%
             select(-utterance, -measure) %>% mutate(src = "data")
           ),
  m.post %>%
    select(MAP, item, bin) %>%
    rename(priorMAP = MAP) %>%
    group_by(item) %>%
    summarize(priorExpectation = sum((bin - 1)*priorMAP))
)

ggplot(mdWonky.wPrior, aes( x = priorExpectation, y = response ) )+
                            #ymin = ci_lower, ymax = ci_upper ))+
  geom_point()+
  facet_wrap(~src)+
  #geom_errorbar(alpha = 0.1)+
  geom_smooth()
```

# Model comparison

Using hierarchical model.

```{r}
rsaModelComparison <- '
var allPriors = myData["priors"]
var allData = myData["data"]

var backoffPrior = mht.fillArray(1, 16);

var domainPrior = function(probs){
  return discrete(probs)
}

// possible utterances
var utterancePrior = function() {
  return uniformDraw(["all", "some", "none"]);
};

// meaning funtion to interpret the utterances
var literalMeanings = {
  all: function(state) { return state === 15; },
  some: function(state) { return state > 0; },
  none: function(state) { return state === 0; }
};

// literal listener
var literalListener = cache(function(utt, prior) {
  return Infer({method:"enumerate"},
  function(){
    var state = domainPrior(prior)
    var meaning = literalMeanings[utt]
    condition(meaning(state))
    return state
  })
}, 10000);

// pragmatic speaker
var speaker = cache(function(state, alpha, prior) {
  return Infer({method:"enumerate"},
  function(){
    var utt = utterancePrior()
    factor(alpha * literalListener(utt, prior).score(state))
    return utt
  })
}, 10000);

// pragmatic listener
var pragmaticListener = function(utt, alpha, wonkyPrior, itemPrior) {
  return Infer({method:"enumerate"},
  function(){
    var wonky = flip(wonkyPrior);
    var prior = wonky ? backoffPrior : itemPrior
    var state = domainPrior(prior)
    observe(speaker(state, alpha, prior), utt)
    return {state: state, wonky: wonky}
  })
}

var items = _.uniq(_.pluck(allPriors, "item"));

var dataAnalysis = function(){
  var wonkyModelDominates = flip(0.5);
  var alpha = uniformDraw(_.range(0.5, 20.5, 0.25));
  display([wonkyModelDominates, alpha])
  //var wonkyPrior = uniformDrift({a:0, b:1, width: 0.1});
  var wonkyPrior = wonkyModelDominates ? 0.5 : 0;
  
  var predictives = map(function(i){
    // webppl objects sorted alphanumerically (so can just pluck)
    // otherwise, would need to ensure that bins correspond to 0-15
    var itemPrior = _.pluck(_.where(allPriors, {item: i}), "MAP");
    var itemData =  _.pluck(_.where(allData, {item: i}), "response");

    var l1Predictions = pragmaticListener("some", alpha, wonkyPrior, itemPrior);

    var l1State = marginalize(l1Predictions, "state");
    var l1Wonky = marginalize(l1Predictions, "wonky");

    observeData({
      data: itemData,
      link: l1State
    })

    // return [ i, {expectation: expectation(l1State), allprob: probability(15, l1State), wonky: expectation(l1Wonky) } ]
  }, items)

  return {wonkyModelDominates: wonkyModelDominates, alpha: alpha}

}
'
```


```{r eval=F}
# load("models/wrsa/results/items_mcmc20k.Rdata")
# d0<-read.csv("bayesian_model_comparison/data/empirical.csv")
# d0.state <- d0 %>% filter(measure == "comp_state")
# dataToPass = list(priors = m.post,
#                   data = d0.state %>% filter(response != 0 ))

bda.results <- webppl(rsaModelComparison, 
       data_var = "myData",
       data = dataToPass,
       packages = c("mht"),
       inference_opts = list(method = "enumerate"),
       model_var = "dataAnalysis")


bda.results %>% select(-alpha) %>% group_by(wonkyModelDominates) %>% 
  summarize(postmodelprob = sum(prob)) %>% 
  spread(wonkyModelDominates, postmodelprob) %>%
  mutate(bf = `TRUE` / `FALSE`)

#save(bda.results,
#      file = "models/wrsa/results/wrsa1_mcmc1kb0.5k_x2.Rdata")
```
